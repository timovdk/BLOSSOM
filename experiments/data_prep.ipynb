{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['random_1', 'random_2', 'random_3', 'random_4', 'random_5', 'random_6', 'random_7', 'random_8', 'random_9', 'random_10', 'random_11', 'random_12', 'random_13', 'random_14', 'random_15', 'random_16', 'random_17', 'random_18', 'random_19', 'random_20', 'clustered_1', 'clustered_2', 'clustered_3', 'clustered_4', 'clustered_5', 'clustered_6', 'clustered_7', 'clustered_8', 'clustered_9', 'clustered_10', 'clustered_11', 'clustered_12', 'clustered_13', 'clustered_14', 'clustered_15', 'clustered_16', 'clustered_17', 'clustered_18', 'clustered_19', 'clustered_20']\n",
    "path = \"../blossom/hpc/outputs/\"\n",
    "rs = [1, 2, 3, 4, 5]\n",
    "init_locs = ['R1', 'R1', 'R1', 'R1', 'R1', 'R1','R1', 'R1', 'R1', 'R1', 'R1', 'R1','R1', 'R1', 'R1', 'R1', 'R1', 'R1','R1', 'R1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1']\n",
    "init_seed  = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "sample_times = [252, 415, 310, 192, 241, 308, 338, 231, 241, 233, 241, 330, 246, 246, 161, 327, 158, 218, 291, 431, 206, 185, 307, 258, 287, 244, 335, 291, 151, 206, 167, 356, 208, 224, 271, 181, 179, 279, 305, 246]\n",
    "x_max = 400\n",
    "y_max = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def systematic_regular(no_of_samples, x, y):\n",
    "    # Calculate the dimensions of the regular grid\n",
    "    rows = int(np.sqrt(no_of_samples))\n",
    "    cols = int(np.ceil(no_of_samples / rows))\n",
    "\n",
    "    # Calculate the padding to ensure points are equally spaced\n",
    "    padding_x = (x - (rows - 1)) // rows\n",
    "    padding_y = (y - (cols - 1)) // cols\n",
    "\n",
    "    # Initialize points list\n",
    "    points = []\n",
    "\n",
    "    # Select equally spaced points with padding\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            points.append((int((i * (padding_x + 1)) + ((padding_x + 1)/2)), int((j * (padding_y + 1) + ((padding_y + 1)/2)))))\n",
    "\n",
    "    return points[:no_of_samples]\n",
    "\n",
    "def wageningen_w():\n",
    "    return [(50, 50), (50, 150), (50, 250), (50, 350), (125, 150), (175, 250), (275, 150), (225, 250), (350, 50), (350, 150), (350, 250), (350, 350)]\n",
    "\n",
    "\n",
    "def retrieve_ids_per_sample_von_neumann(points, data, r):\n",
    "    samples =  [ [] for _ in range(len(points)) ]\n",
    "    for ind, row in data.iterrows():\n",
    "        for index, point in enumerate(points):\n",
    "            x1, y1 = point\n",
    "            x2 = row.x\n",
    "            y2 = row.y\n",
    "\n",
    "            if (abs(x1 - x2) % 400 + abs(y1 - y2) % 400) <= r:\n",
    "                samples[index].append(ind)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts per Sample for each file, for Rs, and Wageningen W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_points_w = wageningen_w()\n",
    "sample_counts_w = pd.DataFrame(columns=['filename', 'init_locs', 'seed', 'diameter','sample_id', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "\n",
    "for idx, filename in enumerate(filenames):  \n",
    "    df = pd.read_csv(path + filename + \".csv\")\n",
    "    data = df[df[\"tick\"] == sample_times[idx]]\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    for r in rs:\n",
    "        samples_w = retrieve_ids_per_sample_von_neumann(selected_points_w, data, r)\n",
    "    \n",
    "        sample_site_counts_w = []\n",
    "        for i, sample in enumerate(samples_w):\n",
    "            df2 = data.iloc[sample]\n",
    "            counts = df2['type'].value_counts().reindex(range(len(df[\"type\"].unique())), fill_value=0)\n",
    "            sample_counts_w = pd.concat([sample_counts_w, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'diameter': [r], 'sample_id': [i], '0': [counts[0]], '1': [counts[1]], '2': [counts[2]], '3': [counts[3]], '4': [counts[4]], '5': [counts[5]], '6': [counts[6]], '7': [counts[7]], '8': [counts[8]]})])\n",
    "\n",
    "sample_counts_w.to_csv('sample_counts_w.csv', index=False)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts per Sample for each file, for Rs, and Sys Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_samples = 16\n",
    "selected_points_reg = systematic_regular(no_of_samples, x_max, y_max)\n",
    "sample_counts_reg = pd.DataFrame(columns=['filename', 'init_locs', 'seed', 'diameter','sample_id', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "\n",
    "for idx, filename in enumerate(filenames):  \n",
    "    df = pd.read_csv(path + filename + \".csv\")\n",
    "    data = df[df[\"tick\"] == sample_times[idx]]\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    for r in rs:\n",
    "        samples_reg = retrieve_ids_per_sample_von_neumann(selected_points_reg, data, r)\n",
    "\n",
    "        sample_site_counts_reg = []\n",
    "        for i, sample in enumerate(samples_reg):\n",
    "            df2 = data.iloc[sample]\n",
    "            counts = df2['type'].value_counts().reindex(range(len(df[\"type\"].unique())), fill_value=0)\n",
    "            sample_counts_reg = pd.concat([sample_counts_reg, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'diameter': [r], 'sample_id': [i], '0': [counts[0]], '1': [counts[1]], '2': [counts[2]], '3': [counts[3]], '4': [counts[4]], '5': [counts[5]], '6': [counts[6]], '7': [counts[7]], '8': [counts[8]]})])\n",
    "            \n",
    "sample_counts_reg.to_csv('sample_counts_reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abundance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1143/2406185393.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  abundances_df = pd.concat([abundances_df, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], '0': counts[0], '1': counts[1], '2': counts[2], '3': counts[3], '4': counts[4], '5': counts[5], '6': counts[6], '7': counts[7], '8': counts[8]})])\n"
     ]
    }
   ],
   "source": [
    "abundances_df = pd.DataFrame(columns=['filename', 'init_locs', 'seed', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "\n",
    "for idx, filename in enumerate(filenames):  \n",
    "    df = pd.read_csv(path + filename + \".csv\")\n",
    "    data = df[df[\"tick\"] == sample_times[idx]]\n",
    "    data = data.reset_index(drop=True)\n",
    "    counts = data['type'].value_counts().reindex(range(len(df[\"type\"].unique())), fill_value=0)\n",
    "    \n",
    "    counts /= 20000\n",
    "    abundances_df = pd.concat([abundances_df, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], '0': counts[0], '1': counts[1], '2': counts[2], '3': counts[3], '4': counts[4], '5': counts[5], '6': counts[6], '7': counts[7], '8': counts[8]})])\n",
    "\n",
    "abundances_df.to_csv('baseline_abundances.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_abundance_estimates(data, num_types, r):\n",
    "    cells = (2*(r**2))+(2*r)+1\n",
    "    sample_weight = cells * 0.125\n",
    "    \n",
    "    norm_abundances_per_sample = []\n",
    "    for i in range(len(data)):\n",
    "        sample = data[data['sample_id'] == i]\n",
    "        abundances_norm = []\n",
    "        for t in range(num_types):\n",
    "            abundances_norm.append(float(sample[str(t)].iloc[0]/sample_weight))\n",
    "        norm_abundances_per_sample.append(abundances_norm)\n",
    "        \n",
    "    return norm_abundances_per_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wageningen W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1143/1899417514.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  abundances_df = pd.concat([abundances_df, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'r': r, 'sample_id': i, '0': sample_abundances[0], '1': sample_abundances[1], '2': sample_abundances[2], '3': sample_abundances[3], '4': sample_abundances[4], '5': sample_abundances[5], '6': sample_abundances[6], '7': sample_abundances[7], '8': sample_abundances[8]})])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./analysis_results/sample_counts_w.csv')\n",
    "abundances_df = pd.DataFrame(columns=['filename', 'init_locs', 'seed', 'r', 'sample_id', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    df1 = data[data['filename'] == filename]\n",
    "    file_abundances = []\n",
    "    for r in rs:\n",
    "        df2 = df1[df1['diameter'] == r]         \n",
    "        plot_abundances_per_sample = compute_abundance_estimates(df2, 9, r)\n",
    "        file_abundances.append(plot_abundances_per_sample)\n",
    "            \n",
    "        for i, sample_abundances in enumerate(plot_abundances_per_sample):\n",
    "            abundances_df = pd.concat([abundances_df, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'r': r, 'sample_id': i, '0': sample_abundances[0], '1': sample_abundances[1], '2': sample_abundances[2], '3': sample_abundances[3], '4': sample_abundances[4], '5': sample_abundances[5], '6': sample_abundances[6], '7': sample_abundances[7], '8': sample_abundances[8]})])\n",
    "            \n",
    "abundances_df.to_csv('estimated_abundances_w.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1143/3500487501.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  abundances_df = pd.concat([abundances_df, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'r': r, 'sample_id': i, '0': sample_abundances[0], '1': sample_abundances[1], '2': sample_abundances[2], '3': sample_abundances[3], '4': sample_abundances[4], '5': sample_abundances[5], '6': sample_abundances[6], '7': sample_abundances[7], '8': sample_abundances[8]})])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./analysis_results/sample_counts_reg.csv')\n",
    "abundances_df = pd.DataFrame(columns=['filename', 'init_locs', 'seed', 'r', 'sample_id', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    df1 = data[data['filename'] == filename]\n",
    "    file_abundances = []\n",
    "    for r in rs:\n",
    "        df2 = df1[df1['diameter'] == r]         \n",
    "        plot_abundances = compute_abundance_estimates(df2, 9, r)\n",
    "        file_abundances.append(plot_abundances)\n",
    "            \n",
    "        for i, sample_abundances in enumerate(plot_abundances):\n",
    "            abundances_df = pd.concat([abundances_df, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'r': r, 'sample_id': i, '0': sample_abundances[0], '1': sample_abundances[1], '2': sample_abundances[2], '3': sample_abundances[3], '4': sample_abundances[4], '5': sample_abundances[5], '6': sample_abundances[6], '7': sample_abundances[7], '8': sample_abundances[8]})])\n",
    "\n",
    "abundances_df.to_csv('estimated_abundances_reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_df = pd.DataFrame(columns=['filename', 'init_locs', 'seed', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "\n",
    "for idx, filename in enumerate(filenames):  \n",
    "    df = pd.read_csv(path + filename + \".csv\")\n",
    "    data = df[df[\"tick\"] == sample_times[idx]]\n",
    "    data = data.reset_index(drop=True)\n",
    "    counts = data['type'].value_counts().reindex(range(len(df[\"type\"].unique())), fill_value=0)\n",
    "    \n",
    "    diversity_df = pd.concat([diversity_df, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], '0': counts[0]>0, '1': counts[1]>0, '2': counts[2]>0, '3': counts[3]>0, '4': counts[4]>0, '5': counts[5]>0, '6': counts[6]>0, '7': counts[7]>0, '8': counts[8]>0})])\n",
    "\n",
    "diversity_df.to_csv('baseline_diversity.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_abundance_estimates(data, num_types, r):\n",
    "    cells = (2*(r**2))+(2*r)+1\n",
    "    sample_weight = cells * 0.125\n",
    "    \n",
    "    norm_abundances_per_sample = []\n",
    "    for i in range(len(data)):\n",
    "        sample = data[data['sample_id'] == i]\n",
    "        abundances_norm = []\n",
    "        for t in range(num_types):\n",
    "            abundances_norm.append(float(sample[str(t)].iloc[0]/sample_weight))\n",
    "        norm_abundances_per_sample.append(abundances_norm)\n",
    "        \n",
    "    return norm_abundances_per_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wageningen W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./analysis_results/sample_counts_w.csv')\n",
    "diversity_df = pd.DataFrame(columns=['filename', 'init_locs', 'seed', 'r', 'sample_id', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    df1 = data[data['filename'] == filename]\n",
    "    file_abundances = []\n",
    "    for r in rs:\n",
    "        df2 = df1[df1['diameter'] == r]         \n",
    "        plot_abundances_per_sample = compute_abundance_estimates(df2, 9, r)\n",
    "        file_abundances.append(plot_abundances_per_sample)\n",
    "            \n",
    "        for i, sample_abundances in enumerate(plot_abundances_per_sample):\n",
    "            diversity_df = pd.concat([diversity_df, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'r': r, 'sample_id': i, '0': sample_abundances[0]>0, '1': sample_abundances[1]>0, '2': sample_abundances[2]>0, '3': sample_abundances[3]>0, '4': sample_abundances[4]>0, '5': sample_abundances[5]>0, '6': sample_abundances[6]>0, '7': sample_abundances[7]>0, '8': sample_abundances[8]>0})])\n",
    "            \n",
    "diversity_df.to_csv('estimated_diversity_w.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./analysis_results/sample_counts_reg.csv')\n",
    "diversity_df = pd.DataFrame(columns=['filename', 'init_locs', 'seed', 'r', 'sample_id', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    df1 = data[data['filename'] == filename]\n",
    "    file_abundances = []\n",
    "    for r in rs:\n",
    "        df2 = df1[df1['diameter'] == r]         \n",
    "        plot_abundances = compute_abundance_estimates(df2, 9, r)\n",
    "        file_abundances.append(plot_abundances)\n",
    "            \n",
    "        for i, sample_abundances in enumerate(plot_abundances):\n",
    "            diversity_df = pd.concat([diversity_df, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'r': r, 'sample_id': i, '0': sample_abundances[0]>0, '1': sample_abundances[1]>0, '2': sample_abundances[2]>0, '3': sample_abundances[3]>0, '4': sample_abundances[4]>0, '5': sample_abundances[5]>0, '6': sample_abundances[6]>0, '7': sample_abundances[7]>0, '8': sample_abundances[8]>0})])\n",
    "\n",
    "diversity_df.to_csv('estimated_diversity_reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_d_index_pairwise(data, grid_dimensions, range_by_type, pseudo_count=1e-5):\n",
    "    X, Y = grid_dimensions\n",
    "    num_types = 9\n",
    "    type_count = np.zeros((X, Y, num_types))  # Counts for each type at each spatial unit\n",
    "    D_matrix = np.zeros((num_types, num_types))  # 9x9 matrix \n",
    "    neighborhood_counts = np.zeros((num_types, num_types))  # Count of neighborhoods for each type pair\n",
    "\n",
    "    # Populate the 2D grid with agent counts\n",
    "    for _, agent in data.iterrows():\n",
    "        x, y, t = agent['x'], agent['y'], agent['type']\n",
    "        type_count[x][y][t] += 1\n",
    "\n",
    "\n",
    "    # Calculate the total number of agents for each type in the entire grid\n",
    "    total_count_by_type = np.sum(type_count, axis=(0, 1))\n",
    "\n",
    "    def compute_neighborhood_counts(x, y, r):\n",
    "        \"\"\"Compute counts of each type in the Von Neumann neighborhood of (x, y) with range r.\"\"\"\n",
    "        neighborhood_count = np.zeros(num_types)\n",
    "        for dx in range(-r, r + 1):\n",
    "            for dy in range(-r + abs(dx), r - abs(dx) + 1):\n",
    "                    nx = (x + dx) % X\n",
    "                    ny = (y + dy) % Y\n",
    "                    neighborhood_count += type_count[nx][ny]\n",
    "        return neighborhood_count\n",
    "\n",
    "    # Calculate the Dissimilarity Index for each spatial unit\n",
    "    for x in range(X):\n",
    "        for y in range(Y):\n",
    "            for t in range(num_types):\n",
    "                r = range_by_type[t]\n",
    "                neighborhood_count_at_unit = compute_neighborhood_counts(x, y, r)\n",
    "                \n",
    "                for t_prime in range(num_types):\n",
    "                    if total_count_by_type[t] > 0 and total_count_by_type[t_prime] > 0:\n",
    "                        prop_t = (neighborhood_count_at_unit[t] + pseudo_count) / total_count_by_type[t]\n",
    "                        prop_t_prime = (neighborhood_count_at_unit[t_prime] + pseudo_count) / total_count_by_type[t_prime]\n",
    "                        \n",
    "                        # Calculate the absolute difference\n",
    "                        D = abs(prop_t - prop_t_prime)\n",
    "                        \n",
    "                        # Accumulate the difference in the respective matrix\n",
    "                        D_matrix[t][t_prime] += D\n",
    "                        neighborhood_counts[t][t_prime] += 1\n",
    "                        if t != t_prime:\n",
    "                            # Ensure symmetric accumulation\n",
    "                            D_matrix[t_prime][t] += D\n",
    "                            neighborhood_counts[t_prime][t] += 1\n",
    "    \n",
    "    # Average the D-index values\n",
    "    for t in range(num_types):\n",
    "        for t_prime in range(num_types):\n",
    "            if neighborhood_counts[t][t_prime] > 0:\n",
    "                # Average the dissimilarity index\n",
    "                D_matrix[t][t_prime] /= neighborhood_counts[t][t_prime]\n",
    "            else:\n",
    "                D_matrix[t][t_prime] = 0  # Handle cases where no neighborhoods were tested\n",
    "\n",
    "    # Normalize by maximum possible dissimilarity\n",
    "    max_possible_dissimilarity = np.max(D_matrix)\n",
    "    if max_possible_dissimilarity > 0:\n",
    "        D_matrix /= max_possible_dissimilarity  # Normalize to [0, 1]\n",
    "    else:\n",
    "        D_matrix = np.zeros_like(D_matrix)  # Handle cases with no data\n",
    "\n",
    "    return D_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersal Range Agent Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44967/2657063668.py:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  indices = pd.concat([indices, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'type_id': [type_id], '0': [row[0]], '1': [row[1]], '2': [row[2]], '3': [row[3]], '4': [row[4]], '5': [row[5]], '6': [row[6]], '7': [row[7]], '8': [row[8]]})])\n"
     ]
    }
   ],
   "source": [
    "indices = pd.DataFrame(columns=['filename', 'init_locs', 'seed', 'type_id', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "d_indices = []\n",
    "for idx, filename in enumerate(filenames):  \n",
    "    df = pd.read_csv(path + filename + \".csv\")\n",
    "    data = df[df[\"tick\"] == sample_times[idx]]\n",
    "    data = data.reset_index(drop=True)\n",
    "    d_index = compute_d_index_pairwise(data, (x_max, y_max), [1,1,3,3,3,4,5,6,6])\n",
    "    for type_id, row in enumerate(d_index):\n",
    "        indices = pd.concat([indices, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'type_id': [type_id], '0': [row[0]], '1': [row[1]], '2': [row[2]], '3': [row[3]], '4': [row[4]], '5': [row[5]], '6': [row[6]], '7': [row[7]], '8': [row[8]]})])\n",
    "    d_indices.append(d_index)\n",
    "\n",
    "indices.to_csv('baseline_d_index_r_org.csv', index=False)\n",
    "arr = np.asarray(d_indices)\n",
    "np.save('baseline_d_index_r_org.npy', arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersal Range 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38938/1165344863.py:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  indices = pd.concat([indices, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'type_id': [type_id], '0': [row[0]], '1': [row[1]], '2': [row[2]], '3': [row[3]], '4': [row[4]], '5': [row[5]], '6': [row[6]], '7': [row[7]], '8': [row[8]]})])\n"
     ]
    }
   ],
   "source": [
    "indices = pd.DataFrame(columns=['filename', 'init_locs', 'seed', 'type_id', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "d_indices = []\n",
    "for idx, filename in enumerate(filenames):  \n",
    "    df = pd.read_csv(path + filename + \".csv\")\n",
    "    data = df[df[\"tick\"] == sample_times[idx]]\n",
    "    data = data.reset_index(drop=True)\n",
    "    d_index = compute_d_index_pairwise(data, (x_max, y_max), [1,1,1,1,1,1,1,1,1])\n",
    "    for type_id, row in enumerate(d_index):\n",
    "        indices = pd.concat([indices, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'type_id': [type_id], '0': [row[0]], '1': [row[1]], '2': [row[2]], '3': [row[3]], '4': [row[4]], '5': [row[5]], '6': [row[6]], '7': [row[7]], '8': [row[8]]})])\n",
    "    d_indices.append(d_index)\n",
    "\n",
    "indices.to_csv('baseline_d_index_r_1.csv', index=False)\n",
    "arr = np.asarray(d_indices)\n",
    "np.save('baseline_d_index_r_1.npy', arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_d_index_pairwise_estimates_sample(counts, num_types, num_samples):\n",
    "    total_count_by_type = []\n",
    "    D_matrix = np.zeros((num_samples, num_types, num_types))\n",
    "    neighborhood_counts = np.zeros((num_samples, num_types, num_types)) \n",
    "    \n",
    "    for idx in range(num_types):\n",
    "        total_count_by_type.append(counts[str(idx)].sum())\n",
    "        \n",
    "    for sample_id in range(len(df2['sample_id'].unique())):\n",
    "        sample_counts = counts[counts['sample_id'] == sample_id]       \n",
    "        for t in range(num_types):  \n",
    "            t_count = sample_counts[str(t)].values[0]\n",
    "            for t_prime in range(num_types):\n",
    "                if t_count > 0 and total_count_by_type[t] > 0 and total_count_by_type[t_prime] > 0:\n",
    "                    prop_t = t_count / total_count_by_type[t]\n",
    "                    prop_t_prime = sample_counts[str(t_prime)].values[0] / total_count_by_type[t_prime]\n",
    "\n",
    "                    # Calculate the absolute difference\n",
    "                    D = abs(prop_t - prop_t_prime)\n",
    "\n",
    "                    # Accumulate the difference in the respective matrix\n",
    "                    D_matrix[sample_id][t][t_prime] += D\n",
    "                    neighborhood_counts[sample_id][t][t_prime] += 1\n",
    "                    if t != t_prime:\n",
    "                        # Ensure symmetric accumulation\n",
    "                        D_matrix[sample_id][t_prime][t] += D\n",
    "                        neighborhood_counts[sample_id][t_prime][t] += 1\n",
    "    \n",
    "    # Average the D-index values\n",
    "    for s in range(num_samples):\n",
    "        for t in range(num_types):\n",
    "            for t_prime in range(num_types):\n",
    "                if neighborhood_counts[s][t][t_prime] > 0:\n",
    "                    # Average the dissimilarity index\n",
    "                    D_matrix[s][t][t_prime] /= neighborhood_counts[s][t][t_prime]\n",
    "                else:\n",
    "                    D_matrix[s][t][t_prime] = 0  # Handle cases where no neighborhoods were tested\n",
    "\n",
    "    # Normalize by maximum possible dissimilarity\n",
    "    max_possible_dissimilarity = np.max(D_matrix)\n",
    "    if max_possible_dissimilarity > 0:\n",
    "        D_matrix /= max_possible_dissimilarity  # Normalize to [0, 1]\n",
    "    else:\n",
    "        D_matrix = np.zeros_like(D_matrix)  # Handle cases with no data\n",
    "\n",
    "    return D_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_d_index_pairwise_estimates_plot(counts, num_types):\n",
    "    total_count_by_type = []\n",
    "    D_matrix = np.zeros((num_types, num_types))\n",
    "    neighborhood_counts = np.zeros((num_types, num_types)) \n",
    "    \n",
    "    for idx in range(num_types):\n",
    "        total_count_by_type.append(counts[str(idx)].sum())\n",
    "        \n",
    "    for sample_id in range(len(df2['sample_id'].unique())):\n",
    "        sample_counts = counts[counts['sample_id'] == sample_id]       \n",
    "        for t in range(num_types):  \n",
    "            t_count = sample_counts[str(t)].values[0]\n",
    "            for t_prime in range(num_types):\n",
    "                if t_count > 0 and total_count_by_type[t] > 0 and total_count_by_type[t_prime] > 0:\n",
    "                    prop_t = t_count / total_count_by_type[t]\n",
    "                    prop_t_prime = sample_counts[str(t_prime)].values[0] / total_count_by_type[t_prime]\n",
    "\n",
    "                    # Calculate the absolute difference\n",
    "                    D = abs(prop_t - prop_t_prime)\n",
    "\n",
    "                    # Accumulate the difference in the respective matrix\n",
    "                    D_matrix[t][t_prime] += D\n",
    "                    neighborhood_counts[t][t_prime] += 1\n",
    "                    if t != t_prime:\n",
    "                        # Ensure symmetric accumulation\n",
    "                        D_matrix[t_prime][t] += D\n",
    "                        neighborhood_counts[t_prime][t] += 1\n",
    "    \n",
    "    # Average the D-index values\n",
    "    for t in range(num_types):\n",
    "        for t_prime in range(num_types):\n",
    "            if neighborhood_counts[t][t_prime] > 0:\n",
    "                # Average the dissimilarity index\n",
    "                D_matrix[t][t_prime] /= neighborhood_counts[t][t_prime]\n",
    "            else:\n",
    "                D_matrix[t][t_prime] = 0  # Handle cases where no neighborhoods were tested\n",
    "\n",
    "    # Normalize by maximum possible dissimilarity\n",
    "    max_possible_dissimilarity = np.max(D_matrix)\n",
    "    if max_possible_dissimilarity > 0:\n",
    "        D_matrix /= max_possible_dissimilarity  # Normalize to [0, 1]\n",
    "    else:\n",
    "        D_matrix = np.zeros_like(D_matrix)  # Handle cases with no data\n",
    "\n",
    "    return D_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wageningen W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132826/628926582.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  indices = pd.concat([indices, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'sample_id': [sample_id], 'type_id': [type_id], 'r': r, '0': [row[0]], '1': [row[1]], '2': [row[2]], '3': [row[3]], '4': [row[4]], '5': [row[5]], '6': [row[6]], '7': [row[7]], '8': [row[8]]})])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./analysis_results/sample_counts_w.csv')\n",
    "indices = pd.DataFrame(columns=['filename', 'init_locs', 'seed', 'sample_id', 'type_id', 'r', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "d_indices = []\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    df1 = data[data['filename'] == filename]\n",
    "    file_indices = []\n",
    "    for r in rs:\n",
    "        df2 = df1[df1['diameter'] == r]         \n",
    "        d_index = compute_d_index_pairwise_estimates_sample(df2, 9, len(df2['sample_id'].unique()))\n",
    "        file_indices.append(d_index)\n",
    "        \n",
    "        for sample_id, rows in enumerate(d_index):\n",
    "            for type_id, row in enumerate(rows):\n",
    "                indices = pd.concat([indices, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'sample_id': [sample_id], 'type_id': [type_id], 'r': r, '0': [row[0]], '1': [row[1]], '2': [row[2]], '3': [row[3]], '4': [row[4]], '5': [row[5]], '6': [row[6]], '7': [row[7]], '8': [row[8]]})])\n",
    "    d_indices.append(file_indices)\n",
    "indices.to_csv('estimated_d_index_sample_w.csv', index=False)\n",
    "arr = np.asarray(d_indices)\n",
    "np.save('estimated_d_index_sample_w.npy', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132826/2492469154.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  indices = pd.concat([indices, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'type_id': [type_id], 'r': r, '0': [row[0]], '1': [row[1]], '2': [row[2]], '3': [row[3]], '4': [row[4]], '5': [row[5]], '6': [row[6]], '7': [row[7]], '8': [row[8]]})])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./analysis_results/sample_counts_w.csv')\n",
    "indices = pd.DataFrame(columns=['filename', 'init_locs', 'seed', 'type_id', 'r', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "d_indices = []\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    df1 = data[data['filename'] == filename]\n",
    "    file_indices = []\n",
    "    for r in rs:\n",
    "        df2 = df1[df1['diameter'] == r]         \n",
    "        d_index = compute_d_index_pairwise_estimates_plot(df2, 9)\n",
    "        file_indices.append(d_index)\n",
    "            \n",
    "        for type_id, row in enumerate(d_index):\n",
    "            indices = pd.concat([indices, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'type_id': [type_id], 'r': r, '0': [row[0]], '1': [row[1]], '2': [row[2]], '3': [row[3]], '4': [row[4]], '5': [row[5]], '6': [row[6]], '7': [row[7]], '8': [row[8]]})])\n",
    "    d_indices.append(file_indices)\n",
    "indices.to_csv('estimated_d_index_plot_w.csv', index=False)\n",
    "arr = np.asarray(d_indices)\n",
    "np.save('estimated_d_index_plot_w.npy', arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132826/3324725479.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  indices = pd.concat([indices, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'sample_id': [sample_id], 'type_id': [type_id], 'r': r, '0': [row[0]], '1': [row[1]], '2': [row[2]], '3': [row[3]], '4': [row[4]], '5': [row[5]], '6': [row[6]], '7': [row[7]], '8': [row[8]]})])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./analysis_results/sample_counts_reg.csv')\n",
    "indices = pd.DataFrame(columns=['filename', 'init_locs', 'seed', 'sample_id', 'type_id', 'r', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "d_indices = []\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    df1 = data[data['filename'] == filename]\n",
    "    file_indices = []\n",
    "    for r in rs:\n",
    "        df2 = df1[df1['diameter'] == r]         \n",
    "        d_index = compute_d_index_pairwise_estimates_sample(df2, 9, len(df2['sample_id'].unique()))\n",
    "        file_indices.append(d_index)\n",
    "        \n",
    "        for sample_id, rows in enumerate(d_index):\n",
    "            for type_id, row in enumerate(rows):\n",
    "                indices = pd.concat([indices, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'sample_id': [sample_id], 'type_id': [type_id], 'r': r, '0': [row[0]], '1': [row[1]], '2': [row[2]], '3': [row[3]], '4': [row[4]], '5': [row[5]], '6': [row[6]], '7': [row[7]], '8': [row[8]]})])\n",
    "    d_indices.append(file_indices)\n",
    "indices.to_csv('estimated_d_index_sample_reg.csv', index=False)\n",
    "arr = np.asarray(d_indices)\n",
    "np.save('estimated_d_index_sample_reg.npy', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132826/2349385515.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  indices = pd.concat([indices, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'type_id': [type_id], 'r': r, '0': [row[0]], '1': [row[1]], '2': [row[2]], '3': [row[3]], '4': [row[4]], '5': [row[5]], '6': [row[6]], '7': [row[7]], '8': [row[8]]})])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./analysis_results/sample_counts_reg.csv')\n",
    "indices = pd.DataFrame(columns=['filename', 'init_locs', 'seed', 'type_id', 'r', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "d_indices = []\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    df1 = data[data['filename'] == filename]\n",
    "    file_indices = []\n",
    "    for r in rs:\n",
    "        df2 = df1[df1['diameter'] == r]         \n",
    "        d_index = compute_d_index_pairwise_estimates_plot(df2, 9)\n",
    "        file_indices.append(d_index)\n",
    "            \n",
    "        for type_id, row in enumerate(d_index):\n",
    "            indices = pd.concat([indices, pd.DataFrame({'filename': [filename], 'init_locs': [init_locs[idx]], 'seed': [init_seed[idx]], 'type_id': [type_id], 'r': r, '0': [row[0]], '1': [row[1]], '2': [row[2]], '3': [row[3]], '4': [row[4]], '5': [row[5]], '6': [row[6]], '7': [row[7]], '8': [row[8]]})])\n",
    "    d_indices.append(file_indices)\n",
    "indices.to_csv('estimated_d_index_plot_reg.csv', index=False)\n",
    "arr = np.asarray(d_indices)\n",
    "np.save('estimated_d_index_plot_reg.npy', arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
