{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "init_types = [\"random\", \"clustered\"]\n",
    "trials = 20\n",
    "filenames = [\n",
    "    f\"{init_type}_{trial}\" for init_type in init_types for trial in range(1, trials + 1)\n",
    "]\n",
    "path = \"./raw_data/\"\n",
    "rs = [1, 2, 3, 4, 5]\n",
    "sample_times = [0, 100, 200, 300, 400, 500, 600]\n",
    "x_max = 400\n",
    "y_max = 400\n",
    "num_types = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Organisms per Type per Simulated Sample (and Time Step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve DataFrame indices that are within Von Neumann neighbourhood defined by centers and r\n",
    "def retrieve_ids_per_sample_von_neumann(centers, data_x, data_y, r):\n",
    "    samples = [[] for _ in range(len(centers))]\n",
    "\n",
    "    for i, (center_x, center_y) in enumerate(centers):\n",
    "        distances = (np.abs(center_x - data_x) % 400) + (\n",
    "            np.abs(center_y - data_y) % 400\n",
    "        )\n",
    "        samples[i] = np.where(distances <= r)[0].tolist()\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Sample centers\n",
    "# fmt: off\n",
    "selected_points_w = [ (50, 50), (50, 150), (50, 250), (50, 350), (125, 150), (175, 250), (275, 150), (225, 250), (350, 50), (350, 150), (350, 250), (350, 350)]\n",
    "selected_points_reg = [ (50, 50), (50, 150), (50, 250), (50, 350), (150, 50), (150, 150), (150, 250), (150, 350), (250, 50), (250, 150), (250, 250), (250, 350), (350, 50), (350, 150), (350, 250), (350, 350)]\n",
    "# fmt: on\n",
    "\n",
    "# Store results in lists\n",
    "sample_counts_w_list = []\n",
    "sample_counts_reg_list = []\n",
    "\n",
    "# Process each file\n",
    "for idx, filename in enumerate(filenames):\n",
    "    df = pl.read_parquet(path + filename + \".parquet\")\n",
    "\n",
    "    data_x = df[\"x\"].to_numpy()\n",
    "    data_y = df[\"y\"].to_numpy()\n",
    "    data_type = df[\"type\"].to_numpy()\n",
    "\n",
    "    for st in sample_times:\n",
    "        # Filter data by time step\n",
    "        mask = df[\"tick\"] == st\n",
    "        filtered_x = data_x[mask]\n",
    "        filtered_y = data_y[mask]\n",
    "        filtered_type = data_type[mask]\n",
    "\n",
    "        for r in rs:\n",
    "            # Get DataFrame indices of organisms within Von Neumann neighbourhood of samples\n",
    "            samples_w = retrieve_ids_per_sample_von_neumann(\n",
    "                selected_points_w, filtered_x, filtered_y, r\n",
    "            )\n",
    "            samples_reg = retrieve_ids_per_sample_von_neumann(\n",
    "                selected_points_reg, filtered_x, filtered_y, r\n",
    "            )\n",
    "\n",
    "            # Count organism types\n",
    "            for i, sample in enumerate(samples_w):\n",
    "                unique, counts = np.unique(filtered_type[sample], return_counts=True)\n",
    "                type_counts = {str(t): 0 for t in range(9)}\n",
    "                type_counts.update(dict(zip(map(str, unique), counts)))\n",
    "\n",
    "                sample_counts_w_list.append(\n",
    "                    {\n",
    "                        \"filename\": filename,\n",
    "                        \"sample_time\": st,\n",
    "                        \"r\": r,\n",
    "                        \"sample_id\": i,\n",
    "                        **type_counts,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            for i, sample in enumerate(samples_reg):\n",
    "                unique, counts = np.unique(filtered_type[sample], return_counts=True)\n",
    "                type_counts = {str(t): 0 for t in range(9)}\n",
    "                type_counts.update(dict(zip(map(str, unique), counts)))\n",
    "\n",
    "                sample_counts_reg_list.append(\n",
    "                    {\n",
    "                        \"filename\": filename,\n",
    "                        \"sample_time\": st,\n",
    "                        \"r\": r,\n",
    "                        \"sample_id\": i,\n",
    "                        **type_counts,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "# Convert lists to DataFrame and save to CSV\n",
    "pl.DataFrame(sample_counts_w_list).write_csv(\"prep_out/sample_counts_w.csv\")\n",
    "pl.DataFrame(sample_counts_reg_list).write_csv(\"prep_out/sample_counts_reg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abundances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the results\n",
    "abundances_list = []\n",
    "\n",
    "# Loop over filenames\n",
    "for idx, filename in enumerate(filenames):\n",
    "    # Read the parquet file using polars\n",
    "    df = pl.read_parquet(path + filename + \".parquet\")\n",
    "\n",
    "    # Loop over sample times\n",
    "    for st in sample_times:\n",
    "        # Filter the data for the current sample time\n",
    "        data = df.filter(pl.col(\"tick\") == st)\n",
    "\n",
    "        # Count occurrences of each type (0-8)\n",
    "        counts = (\n",
    "            data.group_by(\"type\")\n",
    "            .agg(pl.len())\n",
    "            .select(\n",
    "                [pl.col(\"type\"), (pl.col(\"len\") / 20000).round(5).alias(\"abundance\")]\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.col(\"type\").cast(pl.Int32)  # Ensuring 'type' is an integer\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Ensure all organism types (0-8) are included even if missing\n",
    "        # Create a range of types (0-8) and left join with the counts\n",
    "        all_types = pl.DataFrame({\"type\": list(range(9))})\n",
    "        counts = all_types.join(counts, on=\"type\", how=\"left\").fill_null(0)\n",
    "\n",
    "        # Append the result to the list\n",
    "        abundances_list.append(\n",
    "            {\n",
    "                \"filename\": filename,\n",
    "                \"sample_time\": st,\n",
    "                **{\n",
    "                    str(i): counts.filter(pl.col(\"type\") == i)[\"abundance\"].to_list()[0]\n",
    "                    for i in range(9)\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list of dictionaries to a polars DataFrame and then to CSV\n",
    "abundances_df = pl.DataFrame(abundances_list)\n",
    "abundances_df.write_csv(\"prep_out/ground_truth_abundances.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute normalized abundance estimates\n",
    "def compute_abundance_estimates(df, num_types, r):\n",
    "    cells = (2 * (r**2)) + (2 * r) + 1\n",
    "    sample_weight = cells * 0.125\n",
    "\n",
    "    # Normalize and round to 5 decimals\n",
    "    return df.select(\n",
    "        \"filename\",\n",
    "        \"r\",\n",
    "        \"sample_time\",\n",
    "        \"sample_id\",\n",
    "        *[pl.col(str(t)).cast(pl.Float64) / sample_weight for t in range(num_types)],\n",
    "    ).with_columns([pl.col(str(t)).round(5) for t in range(num_types)])\n",
    "\n",
    "\n",
    "data_w = pl.read_csv(\"./prep_out/sample_counts_w.csv\")\n",
    "data_reg = pl.read_csv(\"./prep_out/sample_counts_reg.csv\")\n",
    "\n",
    "results_w = []\n",
    "results_reg = []\n",
    "\n",
    "for filename in filenames:\n",
    "    df1_w = data_w.filter(pl.col(\"filename\") == filename)\n",
    "    df1_reg = data_reg.filter(pl.col(\"filename\") == filename)\n",
    "\n",
    "    for st in sample_times:\n",
    "        df2_w = df1_w.filter(pl.col(\"sample_time\") == st)\n",
    "        df2_reg = df1_reg.filter(pl.col(\"sample_time\") == st)\n",
    "\n",
    "        for r in rs:\n",
    "            df3_w = df2_w.filter(pl.col(\"r\") == r)\n",
    "            df3_reg = df2_reg.filter(pl.col(\"r\") == r)\n",
    "\n",
    "            # Compute normalized abundances\n",
    "            norm_abundances_w = compute_abundance_estimates(df3_w, num_types=9, r=r)\n",
    "            norm_abundances_reg = compute_abundance_estimates(df3_reg, num_types=9, r=r)\n",
    "\n",
    "            # Append to results\n",
    "            results_w.append(norm_abundances_w)\n",
    "            results_reg.append(norm_abundances_reg)\n",
    "\n",
    "# Concatenate all results and save to CSV\n",
    "pl.concat(results_w).write_csv(\"prep_out/estimated_abundances_w.csv\")\n",
    "pl.concat(results_reg).write_csv(\"prep_out/estimated_abundances_reg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diversity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_diversity(counts):\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    proportions = counts / total\n",
    "    return -np.sum(proportions[proportions > 0] * np.log(proportions[proportions > 0]))\n",
    "\n",
    "\n",
    "def simpson_diversity(counts):\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    proportions = counts / total\n",
    "    return 1.0 - np.sum(proportions**2)\n",
    "\n",
    "\n",
    "diversity_results = []\n",
    "\n",
    "for filename in filenames:\n",
    "    df = pl.read_parquet(path + filename + \".parquet\")\n",
    "\n",
    "    for st in sample_times:\n",
    "        data = df.filter(pl.col(\"tick\") == st)\n",
    "\n",
    "        # Compute type counts and reindex to ensure all organism types 0-8 exist\n",
    "        counts = (\n",
    "            data[\"type\"]\n",
    "            .value_counts()\n",
    "            .with_columns(pl.col(\"type\").cast(pl.Int64))\n",
    "            .sort(\"type\")\n",
    "        )\n",
    "\n",
    "        full_range_df = pl.DataFrame({\"type\": range(9)})\n",
    "        counts_df = full_range_df.join(counts, on=\"type\", how=\"left\").with_columns(\n",
    "            pl.col(\"count\").fill_null(0)\n",
    "        )\n",
    "\n",
    "        counts_array = counts_df[\"count\"].to_numpy()\n",
    "        diversity_results.append(\n",
    "            (\n",
    "                filename,\n",
    "                st,\n",
    "                shannon_diversity(counts_array).round(5),\n",
    "                simpson_diversity(counts_array).round(5),\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Convert results to Polars DataFrame and save\n",
    "pl.DataFrame(\n",
    "    diversity_results,\n",
    "    schema=[\"filename\", \"sample_time\", \"shannon\", \"simpson\"],\n",
    "    orient=\"row\",\n",
    ").write_csv(\"prep_out/ground_truth_diversity_indices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_diversity(counts):\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    proportions = counts / total\n",
    "    return -np.sum(proportions[proportions > 0] * np.log(proportions[proportions > 0]))\n",
    "\n",
    "\n",
    "def simpson_diversity(counts):\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    proportions = counts / total\n",
    "    return 1.0 - np.sum(proportions**2)\n",
    "\n",
    "\n",
    "df_w = pl.read_csv(\"prep_out/sample_counts_w.csv\")\n",
    "df_reg = pl.read_csv(\"prep_out/sample_counts_reg.csv\")\n",
    "\n",
    "count_cols = [str(i) for i in range(9)]\n",
    "diversity_dfs_w = []\n",
    "diversity_dfs_reg = []\n",
    "\n",
    "for group_cols, output_file in [\n",
    "    (\n",
    "        [\"filename\", \"r\", \"sample_time\", \"sample_id\"],\n",
    "        \"estimated_diversity_indices_sample\",\n",
    "    ),  # No pooling\n",
    "    ([\"filename\", \"r\", \"sample_time\"], \"estimated_diversity_indices_plot\"),  # Per plot\n",
    "    ([\"filename\", \"r\"], \"estimated_diversity_indices_temporal\"),  # Per plot over time\n",
    "]:\n",
    "    grouped_df_w = df_w.group_by(group_cols).agg(\n",
    "        [pl.sum(col).alias(col) for col in count_cols]\n",
    "    )\n",
    "    grouped_df_reg = df_reg.group_by(group_cols).agg(\n",
    "        [pl.sum(col).alias(col) for col in count_cols]\n",
    "    )\n",
    "\n",
    "    counts_array_w = grouped_df_w.select(count_cols).to_numpy()\n",
    "    counts_array_reg = grouped_df_reg.select(count_cols).to_numpy()\n",
    "\n",
    "    shannon_vals_w = np.round(\n",
    "        np.apply_along_axis(shannon_diversity, 1, counts_array_w), 5\n",
    "    )\n",
    "    simpson_vals_w = np.round(\n",
    "        np.apply_along_axis(simpson_diversity, 1, counts_array_w), 5\n",
    "    )\n",
    "    shannon_vals_reg = np.round(\n",
    "        np.apply_along_axis(shannon_diversity, 1, counts_array_reg), 5\n",
    "    )\n",
    "    simpson_vals_reg = np.round(\n",
    "        np.apply_along_axis(simpson_diversity, 1, counts_array_reg), 5\n",
    "    )\n",
    "\n",
    "    diversity_dfs_w.append(\n",
    "        grouped_df_w.with_columns(\n",
    "            [pl.Series(\"shannon\", shannon_vals_w), pl.Series(\"simpson\", simpson_vals_w)]\n",
    "        )\n",
    "        .drop(count_cols)\n",
    "        .sort(group_cols)  # Drop organism count columns\n",
    "    )\n",
    "    diversity_dfs_reg.append(\n",
    "        grouped_df_reg.with_columns(\n",
    "            [\n",
    "                pl.Series(\"shannon\", shannon_vals_reg),\n",
    "                pl.Series(\"simpson\", simpson_vals_reg),\n",
    "            ]\n",
    "        )\n",
    "        .drop(count_cols)\n",
    "        .sort(group_cols)  # Drop organism count columns\n",
    "    )\n",
    "\n",
    "    diversity_dfs_w[-1].write_csv(f\"prep_out/{output_file}_w.csv\")\n",
    "    diversity_dfs_reg[-1].write_csv(f\"prep_out/{output_file}_reg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_diversity(counts):\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    proportions = counts / total\n",
    "    return -np.sum(proportions[proportions > 0] * np.log(proportions[proportions > 0]))\n",
    "\n",
    "def simpson_diversity(counts):\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    proportions = counts / total\n",
    "    return 1.0 - np.sum(proportions**2)\n",
    "\n",
    "df_w = pl.read_csv(\"prep_out/sample_counts_w.csv\")\n",
    "df_reg = pl.read_csv(\"prep_out/sample_counts_reg.csv\")\n",
    "\n",
    "count_cols = [str(i) for i in range(9)]\n",
    "\n",
    "### 1. Get unique sample IDs sorted\n",
    "sorted_sample_ids = df_w[\"sample_id\"].unique(maintain_order=True)\n",
    "\n",
    "### 2. Initialize result lists\n",
    "sequential_results_w = []\n",
    "sequential_results_reg = []\n",
    "\n",
    "### 3. Iterate over increasing subsets of sample IDs\n",
    "for i in range(1, len(sorted_sample_ids) + 1):\n",
    "    subset_ids = sorted_sample_ids[:i]  # Take the first 'i' samples\n",
    "\n",
    "    # Filter DataFrames for the current subset of sample IDs\n",
    "    subset_df_w = df_w.filter(pl.col(\"sample_id\").is_in(subset_ids))\n",
    "    subset_df_reg = df_reg.filter(pl.col(\"sample_id\").is_in(subset_ids))\n",
    "\n",
    "    # Aggregate data at the (filename, r, sample_time, sample_id) level\n",
    "    grouped_df_w = subset_df_w.group_by([\"filename\", \"r\", \"sample_time\", \"sample_id\"]).agg(\n",
    "        [pl.sum(col).alias(col) for col in count_cols]\n",
    "    )\n",
    "    grouped_df_reg = subset_df_reg.group_by([\"filename\", \"r\", \"sample_time\", \"sample_id\"]).agg(\n",
    "        [pl.sum(col).alias(col) for col in count_cols]\n",
    "    )\n",
    "\n",
    "    # Compute Shannon & Simpson diversity\n",
    "    shannon_vals_w = np.round(\n",
    "        np.apply_along_axis(shannon_diversity, 1, grouped_df_w.select(count_cols).to_numpy()), 5\n",
    "    )\n",
    "    simpson_vals_w = np.round(\n",
    "        np.apply_along_axis(simpson_diversity, 1, grouped_df_w.select(count_cols).to_numpy()), 5\n",
    "    )\n",
    "    shannon_vals_reg = np.round(\n",
    "        np.apply_along_axis(shannon_diversity, 1, grouped_df_reg.select(count_cols).to_numpy()), 5\n",
    "    )\n",
    "    simpson_vals_reg = np.round(\n",
    "        np.apply_along_axis(simpson_diversity, 1, grouped_df_reg.select(count_cols).to_numpy()), 5\n",
    "    )\n",
    "\n",
    "    # Append results\n",
    "    sequential_results_w.append(\n",
    "        grouped_df_w.with_columns(\n",
    "            [pl.Series(\"shannon\", shannon_vals_w), pl.Series(\"simpson\", simpson_vals_w)]\n",
    "        ).drop(count_cols)\n",
    "    )\n",
    "    sequential_results_reg.append(\n",
    "        grouped_df_reg.with_columns(\n",
    "            [pl.Series(\"shannon\", shannon_vals_reg), pl.Series(\"simpson\", simpson_vals_reg)]\n",
    "        ).drop(count_cols)\n",
    "    )\n",
    "\n",
    "### 4. Concatenate results and save\n",
    "final_df_w = pl.concat(sequential_results_w).sort([\"filename\", \"r\", \"sample_time\", \"sample_id\"])\n",
    "final_df_reg = pl.concat(sequential_results_reg).sort([\"filename\", \"r\", \"sample_time\", \"sample_id\"])\n",
    "\n",
    "final_df_w.write_csv(\"prep_out/estimated_diversity_indices_n_samples_seq_w.csv\")\n",
    "final_df_reg.write_csv(\"prep_out/estimated_diversity_indices_n_samples_seq_reg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shannon_diversity(counts):\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    proportions = counts / total\n",
    "    return -np.sum(proportions[proportions > 0] * np.log(proportions[proportions > 0]))\n",
    "\n",
    "def simpson_diversity(counts):\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    proportions = counts / total\n",
    "    return 1.0 - np.sum(proportions**2)\n",
    "\n",
    "df_w = pl.read_csv(\"prep_out/sample_counts_w.csv\")\n",
    "df_reg = pl.read_csv(\"prep_out/sample_counts_reg.csv\")\n",
    "\n",
    "count_cols = [str(i) for i in range(9)]\n",
    "\n",
    "### 1. Get unique sample IDs sorted\n",
    "sorted_sample_ids = df_w[\"sample_id\"].unique(maintain_order=True)\n",
    "\n",
    "### 2. Initialize result lists\n",
    "sequential_results_w = []\n",
    "sequential_results_reg = []\n",
    "\n",
    "### 3. Iterate over increasing subsets of sample IDs\n",
    "for i in range(1, len(sorted_sample_ids) + 1):\n",
    "    subset_ids = random.sample(list(sorted_sample_ids), i)\n",
    "\n",
    "    # Filter DataFrames for the current subset of sample IDs\n",
    "    subset_df_w = df_w.filter(pl.col(\"sample_id\").is_in(subset_ids))\n",
    "    subset_df_reg = df_reg.filter(pl.col(\"sample_id\").is_in(subset_ids))\n",
    "\n",
    "    # Aggregate data at the (filename, r, sample_time, sample_id) level\n",
    "    grouped_df_w = subset_df_w.group_by([\"filename\", \"r\", \"sample_time\", \"sample_id\"]).agg(\n",
    "        [pl.sum(col).alias(col) for col in count_cols]\n",
    "    )\n",
    "    grouped_df_reg = subset_df_reg.group_by([\"filename\", \"r\", \"sample_time\", \"sample_id\"]).agg(\n",
    "        [pl.sum(col).alias(col) for col in count_cols]\n",
    "    )\n",
    "\n",
    "    # Compute Shannon & Simpson diversity\n",
    "    shannon_vals_w = np.round(\n",
    "        np.apply_along_axis(shannon_diversity, 1, grouped_df_w.select(count_cols).to_numpy()), 5\n",
    "    )\n",
    "    simpson_vals_w = np.round(\n",
    "        np.apply_along_axis(simpson_diversity, 1, grouped_df_w.select(count_cols).to_numpy()), 5\n",
    "    )\n",
    "    shannon_vals_reg = np.round(\n",
    "        np.apply_along_axis(shannon_diversity, 1, grouped_df_reg.select(count_cols).to_numpy()), 5\n",
    "    )\n",
    "    simpson_vals_reg = np.round(\n",
    "        np.apply_along_axis(simpson_diversity, 1, grouped_df_reg.select(count_cols).to_numpy()), 5\n",
    "    )\n",
    "\n",
    "    # Append results\n",
    "    sequential_results_w.append(\n",
    "        grouped_df_w.with_columns(\n",
    "            [pl.Series(\"shannon\", shannon_vals_w), pl.Series(\"simpson\", simpson_vals_w)]\n",
    "        ).drop(count_cols)\n",
    "    )\n",
    "    sequential_results_reg.append(\n",
    "        grouped_df_reg.with_columns(\n",
    "            [pl.Series(\"shannon\", shannon_vals_reg), pl.Series(\"simpson\", simpson_vals_reg)]\n",
    "        ).drop(count_cols)\n",
    "    )\n",
    "\n",
    "### 4. Concatenate results and save\n",
    "final_df_w = pl.concat(sequential_results_w).sort([\"filename\", \"r\", \"sample_time\", \"sample_id\"])\n",
    "final_df_reg = pl.concat(sequential_results_reg).sort([\"filename\", \"r\", \"sample_time\", \"sample_id\"])\n",
    "\n",
    "final_df_w.write_csv(\"prep_out/estimated_diversity_indices_n_samples_rand_w.csv\")\n",
    "final_df_reg.write_csv(\"prep_out/estimated_diversity_indices_n_samples_rand_reg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D-Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ground_truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:18<00:00, 78.54s/it]\n"
     ]
    }
   ],
   "source": [
    "def compute_d_index_pairwise(data, range_by_type, pseudo_count=1e-5):\n",
    "    type_count = np.zeros((x_max, y_max, num_types))  # (x, y, type)\n",
    "    D_matrix = np.zeros((num_types, num_types))  # 9x9 dissimilarity matrix\n",
    "    neighborhood_counts = np.zeros((num_types, num_types))  # Neighborhood count tracker\n",
    "\n",
    "    # Populate the 2D grid with agent counts (adjusted for multiple organisms in the same location)\n",
    "    x_vals, y_vals, type_vals = (\n",
    "        data[\"x\"].to_numpy(),\n",
    "        data[\"y\"].to_numpy(),\n",
    "        data[\"type\"].to_numpy(),\n",
    "    )\n",
    "    for x, y, t in zip(x_vals, y_vals, type_vals):\n",
    "        type_count[x, y, t] += (\n",
    "            1  # Increment the count for each organism type at the (x, y) location\n",
    "        )\n",
    "\n",
    "    # Calculate the total number of agents for each type in the entire grid\n",
    "    total_count_by_type = np.sum(type_count, axis=(0, 1))\n",
    "\n",
    "    # Vectorized neighborhood count function\n",
    "    def compute_neighborhood_counts(x, y, r):\n",
    "        \"\"\"Vectorized Von Neumann neighborhood count computation.\"\"\"\n",
    "        neighborhood_count = np.zeros(num_types)\n",
    "        for dx in range(-r, r + 1):\n",
    "            for dy in range(-r + abs(dx), r - abs(dx) + 1):\n",
    "                nx = (x + dx) % x_max\n",
    "                ny = (y + dy) % y_max\n",
    "                neighborhood_count += type_count[nx, ny]\n",
    "        return neighborhood_count\n",
    "\n",
    "    # Compute D-index\n",
    "    for x in range(x_max):\n",
    "        for y in range(y_max):\n",
    "            for t in range(num_types):\n",
    "                r = range_by_type[t]\n",
    "                neighborhood_count_at_unit = compute_neighborhood_counts(x, y, r)\n",
    "\n",
    "                for t_prime in range(num_types):\n",
    "                    if total_count_by_type[t] > 0 and total_count_by_type[t_prime] > 0:\n",
    "                        prop_t = (\n",
    "                            neighborhood_count_at_unit[t] + pseudo_count\n",
    "                        ) / total_count_by_type[t]\n",
    "                        prop_t_prime = (\n",
    "                            neighborhood_count_at_unit[t_prime] + pseudo_count\n",
    "                        ) / total_count_by_type[t_prime]\n",
    "\n",
    "                        # Compute absolute difference\n",
    "                        D = abs(prop_t - prop_t_prime)\n",
    "\n",
    "                        # Accumulate values symmetrically\n",
    "                        D_matrix[t, t_prime] += D\n",
    "                        neighborhood_counts[t, t_prime] += 1\n",
    "\n",
    "    # Normalize D-index\n",
    "    mask = neighborhood_counts > 0\n",
    "    D_matrix[mask] /= neighborhood_counts[mask]  # Avoid division by zero\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "    max_D = np.max(D_matrix)\n",
    "    if max_D > 0:\n",
    "        D_matrix /= max_D\n",
    "\n",
    "    return D_matrix\n",
    "\n",
    "\n",
    "d_index_results = []\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    df = pl.read_parquet(path + filename + \".parquet\")\n",
    "\n",
    "    for st in sample_times:\n",
    "        data = df.filter(pl.col(\"tick\") == st)\n",
    "\n",
    "        # Compute the D-index (using the optimized function)\n",
    "        d_index = compute_d_index_pairwise(data, [1] * 9)\n",
    "\n",
    "        # Store results efficiently in a list\n",
    "        for type_id, row in enumerate(d_index):\n",
    "            rounded_row = tuple(\n",
    "                round(val, 5) for val in row\n",
    "            )  # Round each value to 5 decimals\n",
    "            d_index_results.append((filename, st, type_id, *rounded_row))\n",
    "\n",
    "# Convert list to a Polars DataFrame in one step (much faster than looping `concat`)\n",
    "indices_df = pl.DataFrame(\n",
    "    d_index_results,\n",
    "    schema=[\"filename\", \"sample_time\", \"type_id\"] + [str(i) for i in range(9)],\n",
    "    orient=\"row\",\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "indices_df.write_csv(\"prep_out/ground_truth_d_index_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_d_index(counts, num_samples, mode):\n",
    "    total_count_by_type = [counts[str(idx)].sum() for idx in range(num_types)]\n",
    "\n",
    "    if mode == \"sample\":\n",
    "        D_matrix = np.zeros((num_samples, num_types, num_types))\n",
    "        neighborhood_counts = np.zeros((num_samples, num_types, num_types))\n",
    "    else:  # \"plot\"\n",
    "        D_matrix = np.zeros((num_types, num_types))\n",
    "        neighborhood_counts = np.zeros((num_types, num_types))\n",
    "\n",
    "    for sample_id in range(num_samples):\n",
    "        sample_counts = counts.filter(pl.col(\"sample_id\") == sample_id)\n",
    "        for t in range(num_types):\n",
    "            t_count = sample_counts[str(t)].sum()\n",
    "            for t_prime in range(num_types):\n",
    "                if all(\n",
    "                    [\n",
    "                        t_count > 0,\n",
    "                        total_count_by_type[t] > 0,\n",
    "                        total_count_by_type[t_prime] > 0,\n",
    "                    ]\n",
    "                ):\n",
    "                    prop_t = t_count / total_count_by_type[t]\n",
    "                    prop_t_prime = (\n",
    "                        sample_counts[str(t_prime)].sum() / total_count_by_type[t_prime]\n",
    "                    )\n",
    "                    D = abs(prop_t - prop_t_prime)\n",
    "\n",
    "                    if mode == \"sample\":\n",
    "                        D_matrix[sample_id, t, t_prime] += D\n",
    "                        neighborhood_counts[sample_id, t, t_prime] += 1\n",
    "                        if t != t_prime:\n",
    "                            D_matrix[sample_id, t_prime, t] += D\n",
    "                            neighborhood_counts[sample_id, t_prime, t] += 1\n",
    "                    else:\n",
    "                        D_matrix[t, t_prime] += D\n",
    "                        neighborhood_counts[t, t_prime] += 1\n",
    "                        if t != t_prime:\n",
    "                            D_matrix[t_prime, t] += D\n",
    "                            neighborhood_counts[t_prime, t] += 1\n",
    "\n",
    "    valid_counts = neighborhood_counts > 0\n",
    "    D_matrix[valid_counts] /= neighborhood_counts[valid_counts]\n",
    "    D_matrix[~valid_counts] = 0\n",
    "\n",
    "    max_dissimilarity = np.max(D_matrix)\n",
    "    if max_dissimilarity > 0:\n",
    "        D_matrix /= max_dissimilarity\n",
    "    else:\n",
    "        D_matrix.fill(0)\n",
    "\n",
    "    return D_matrix\n",
    "\n",
    "\n",
    "def collect_indices_from_d_index(data, filenames, sample_times, rs, mode, sample_sim):\n",
    "    if mode == \"temporal\":\n",
    "        data = data.group_by([\"filename\", \"r\", \"sample_id\"]).agg(\n",
    "            [pl.col(str(i)).sum().alias(str(i)) for i in range(9)]\n",
    "        )\n",
    "    indices = []\n",
    "    for filename in filenames:\n",
    "        for r in rs:\n",
    "            df_filtered = data.filter(pl.col(\"filename\") == filename).filter(\n",
    "                pl.col(\"r\") == r\n",
    "            )\n",
    "            if mode != \"temporal\":\n",
    "                for st in sample_times:\n",
    "                    df_st_filtered = df_filtered.filter(pl.col(\"sample_time\") == st)\n",
    "                    d_index_result = compute_d_index(\n",
    "                        df_st_filtered,\n",
    "                        len(df_st_filtered[\"sample_id\"].unique()),\n",
    "                        mode=mode,\n",
    "                    )\n",
    "                    if mode == \"sample\":\n",
    "                        for sample_index, sample_id in enumerate(\n",
    "                            df_st_filtered[\"sample_id\"].unique(maintain_order=True)\n",
    "                        ):\n",
    "                            for type_id in range(9):\n",
    "                                row = d_index_result[sample_index, type_id]\n",
    "                                indices.append(\n",
    "                                    {\n",
    "                                        \"filename\": filename,\n",
    "                                        \"r\": r,\n",
    "                                        \"sample_time\": st,\n",
    "                                        \"sample_id\": sample_id,\n",
    "                                        \"type_id\": type_id,\n",
    "                                        **{str(i): row[i].round(5) for i in range(9)},\n",
    "                                    }\n",
    "                                )\n",
    "\n",
    "                    elif mode == \"plot\":\n",
    "                        for type_id, row in enumerate(d_index_result):\n",
    "                            indices.append(\n",
    "                                {\n",
    "                                    \"filename\": filename,\n",
    "                                    \"r\": r,\n",
    "                                    \"sample_time\": st,\n",
    "                                    \"type_id\": type_id,\n",
    "                                    **{str(i): row[i].round(5) for i in range(9)},\n",
    "                                }\n",
    "                            )\n",
    "            else:\n",
    "                d_index_result = compute_d_index(\n",
    "                    df_filtered, len(df_filtered[\"sample_id\"].unique()), mode=\"plot\"\n",
    "                )\n",
    "                for type_id, row in enumerate(d_index_result):\n",
    "                    indices.append(\n",
    "                        {\n",
    "                            \"filename\": filename,\n",
    "                            \"r\": r,\n",
    "                            \"type_id\": type_id,\n",
    "                            **{str(i): row[i].round(5) for i in range(9)},\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    df_result = pl.DataFrame(indices)\n",
    "    df_result = pl.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                k: str(v) if isinstance(v, (list, np.ndarray)) else v\n",
    "                for k, v in entry.items()\n",
    "            }\n",
    "            for entry in indices\n",
    "        ]\n",
    "    )\n",
    "    df_result.write_csv(f\"prep_out/estimated_d_index_{mode}_{sample_sim}.csv\")\n",
    "\n",
    "\n",
    "# Load data\n",
    "data_w = pl.read_csv(\"./prep_out/sample_counts_w.csv\")\n",
    "data_reg = pl.read_csv(\"./prep_out/sample_counts_reg.csv\")\n",
    "\n",
    "# Compute D-index with different pooling strategies\n",
    "collect_indices_from_d_index(\n",
    "    data_w, filenames, sample_times, rs, mode=\"sample\", sample_sim=\"w\"\n",
    ")\n",
    "collect_indices_from_d_index(\n",
    "   data_w, filenames, sample_times, rs, mode=\"plot\", sample_sim=\"w\"\n",
    ")\n",
    "collect_indices_from_d_index(\n",
    "   data_w, filenames, sample_times, rs, mode=\"temporal\", sample_sim=\"w\"\n",
    ")\n",
    "\n",
    "collect_indices_from_d_index(\n",
    "    data_reg, filenames, sample_times, rs, mode=\"sample\", sample_sim=\"reg\"\n",
    ")\n",
    "collect_indices_from_d_index(\n",
    "   data_reg, filenames, sample_times, rs, mode=\"plot\", sample_sim=\"reg\"\n",
    ")\n",
    "collect_indices_from_d_index(\n",
    "   data_reg, filenames, sample_times, rs, mode=\"temporal\", sample_sim=\"reg\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blossom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
