{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I did according to CV\n",
    "- Single soil sample vs W shape (12 samples)\n",
    "\n",
    "### What CV wants\n",
    "- Sample 10 points, pool them (Like W) or not (single samples), are there different results?\n",
    "- If I pool, subsample of ground_truth, pooled sample things will co-occur that did not co-occur in seperate samples (no pooling)\n",
    "- What does this imply for downstream analyses, e.g, orgs co-occur (pooling conclusion) even though they came from different samples (no-pooling)\n",
    "- Analysis of N individual samples is not individual, but they're pooled after they become data. So they should be much better (and more expensive) than pooling before analysis. N is variable, so according to CV N=1 is done\n",
    "\n",
    "### What I did\n",
    "- Abundance (three steps are done in `data_analysis`)\n",
    "    1. 1 sample vs ground_truth (N=1)\n",
    "        - Metrics per `sample_id, sample_time, r`\n",
    "    2. pooled sample vs ground_truth (pooling done by taking mean of x samples)\n",
    "        - Metrics per `sample_time, r`\n",
    "    3. pooled sample vs ground_truth (pooling done by taking mean of x samples, for y sample times)\n",
    "        - Metrics per `r`\n",
    "- Diversity (three steps are done in `data_prep`)\n",
    "    1. 1 Sample, no pooling, diversity indices per sample (N=1)\n",
    "        - Metrics per `sample_id, sample_time, r`\n",
    "    2. Plot, pooling done by aggregating org occs in x samples\n",
    "        - Metrics per `sample_time, r`\n",
    "    3. Temporal, pooling done by aggregating org occs in x samples for y sample times\n",
    "        - Metrics per `r`\n",
    "- D-Index (three steps are done in `data_prep`)\n",
    "    1. 1 Sample, no pooling, D-Index matrix per sample (N=1)\n",
    "        - Metrics per `sample_id, sample_time, r`\n",
    "    2. Plot, pooling done by adding all D-Matrices for x samples, and normalizing\n",
    "        - Metrics per `sample_time, r`\n",
    "    3. Temporal, pooling done by aggregating org occs y sample times PER sample, then adding all D-Matrices for x samples, and normalizing\n",
    "        - Metrics per `r`\n",
    "\n",
    "\n",
    "### What we need to do\n",
    "Fill the gap between steps 1 and 2, and possibly fix step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_normalized_absolute_error(y_true, y_pred, pop_counts):\n",
    "    np.seterr(invalid=\"raise\", divide=\"raise\")\n",
    "    y_true, y_pred, pop_counts = map(np.array, (y_true, y_pred, pop_counts))\n",
    "    absolute_errors = np.abs(y_pred - y_true)\n",
    "\n",
    "    normalized_errors = np.zeros_like(absolute_errors)\n",
    "    valid_indices = pop_counts != 0\n",
    "    if not np.any(valid_indices):\n",
    "        return normalized_errors\n",
    "    normalized_errors[valid_indices] = (\n",
    "        absolute_errors[valid_indices] / pop_counts[valid_indices]\n",
    "    )\n",
    "    return normalized_errors\n",
    "\n",
    "\n",
    "def calculate_normalized_error(y_true, y_pred, pop_counts):\n",
    "    y_true, y_pred, pop_counts = map(np.array, (y_true, y_pred, pop_counts))\n",
    "    errors = y_pred - y_true\n",
    "\n",
    "    normalized_errors = np.zeros_like(errors)\n",
    "    valid_indices = pop_counts != 0\n",
    "    if not np.any(valid_indices):\n",
    "        return normalized_errors\n",
    "    normalized_errors[valid_indices] = errors[valid_indices] / pop_counts[valid_indices]\n",
    "    return normalized_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abundance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metrics_data_sample(\n",
    "    es_r_r,\n",
    "    es_w_r,\n",
    "    bl_st_ab,\n",
    "    values,\n",
    "    sample_time,\n",
    "    file,\n",
    "    rs,\n",
    "    metrics_reg_list,\n",
    "    metrics_w_list,\n",
    "    metrics_reg_type_list,\n",
    "    metrics_w_type_list,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to process and append metrics for both reg and w data\n",
    "    \"\"\"\n",
    "    for r in rs:\n",
    "        es_r_r_filtered = es_r_r.filter(pl.col(\"r\") == r)\n",
    "        es_w_r_filtered = es_w_r.filter(pl.col(\"r\") == r)\n",
    "\n",
    "        for es, metrics_list, metrics_type_list in [\n",
    "            (es_r_r_filtered, metrics_reg_list, metrics_reg_type_list),\n",
    "            (es_w_r_filtered, metrics_w_list, metrics_w_type_list),\n",
    "        ]:\n",
    "            for i in es[\"sample_id\"].unique():\n",
    "                es_ab = (\n",
    "                    es.filter(pl.col(\"sample_id\") == i)\n",
    "                    .select(pl.col([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]))\n",
    "                    .to_numpy()\n",
    "                    .flatten()\n",
    "                    * 1000\n",
    "                )\n",
    "\n",
    "                # Calculate both MAE (normalized absolute error) and MSE (normalized error)\n",
    "                ae = calculate_normalized_absolute_error([bl_st_ab], es_ab, [values])[0]\n",
    "                mse = calculate_normalized_error([bl_st_ab], es_ab, [values])[0]\n",
    "\n",
    "                metrics_list.append(\n",
    "                    {\n",
    "                        \"filename\": file,\n",
    "                        \"sample_time\": sample_time,\n",
    "                        \"r\": r,\n",
    "                        \"sample_id\": i,\n",
    "                        \"mae\": np.mean(ae),\n",
    "                        \"mdae\": np.median(ae),\n",
    "                        \"mser\": np.mean(mse),\n",
    "                        \"mdser\": np.median(mse),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                for t in range(9):\n",
    "                    es_ab_t = es_ab[t]\n",
    "\n",
    "                    ae = calculate_normalized_absolute_error(\n",
    "                        [bl_st_ab[t]], [es_ab_t], [values[t]]\n",
    "                    )[0]\n",
    "                    ser = calculate_normalized_error(\n",
    "                        [bl_st_ab[t]], [es_ab_t], [values[t]]\n",
    "                    )[0]\n",
    "                    metrics_type_list.append(\n",
    "                        {\n",
    "                            \"filename\": file,\n",
    "                            \"sample_time\": sample_time,\n",
    "                            \"r\": r,\n",
    "                            \"sample_id\": i,\n",
    "                            \"type_id\": t,\n",
    "                            \"ae\": ae,\n",
    "                            \"ser\": ser,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "\n",
    "def process_raw_data(\n",
    "    ground_truth_dict,\n",
    "    estimate_reg_dict,\n",
    "    estimate_w_dict,\n",
    "    rs=[1, 2, 3, 4, 5],\n",
    "    sample_times=[0, 100, 200, 300, 400, 500, 600],\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to process raw data for a given sample time and calculate error metrics\n",
    "    \"\"\"\n",
    "    metrics_reg_list = []\n",
    "    metrics_w_list = []\n",
    "    metrics_reg_type_list = []\n",
    "    metrics_w_type_list = []\n",
    "\n",
    "    # Read raw data, ground_truth, and estimate data for the given file and sample time\n",
    "    for file in ground_truth_dict.keys():\n",
    "        raw_data = pl.read_parquet(f\"./raw_data/{file}.parquet\")\n",
    "        for sample_time in sample_times:\n",
    "            raw_data_st = raw_data.filter(pl.col(\"tick\") == sample_time)\n",
    "            bl = ground_truth_dict[file]\n",
    "            es_r = estimate_reg_dict[file]\n",
    "            es_w = estimate_w_dict[file]\n",
    "\n",
    "            # Precompute unique types and their counts\n",
    "            type_counts = raw_data_st.group_by(\"type\").agg(\n",
    "                pl.col(\"type\").count().alias(\"count\")\n",
    "            )\n",
    "            unique_types = raw_data[\"type\"].unique(maintain_order=True)\n",
    "\n",
    "            # Create a fast lookup array for type counts\n",
    "            values = np.zeros(len(unique_types), dtype=int)\n",
    "            values[np.searchsorted(unique_types, type_counts[\"type\"])] = type_counts[\n",
    "                \"count\"\n",
    "            ].to_numpy()\n",
    "\n",
    "            es_st_r = es_r.filter(pl.col(\"sample_time\") == sample_time)\n",
    "            es_st_w = es_w.filter(pl.col(\"sample_time\") == sample_time)\n",
    "            bl_st = bl.filter(pl.col(\"sample_time\") == sample_time)\n",
    "\n",
    "            bl_st_ab = (\n",
    "                bl_st.select(pl.col([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]))\n",
    "                .to_numpy()\n",
    "                .flatten()\n",
    "                * 1000\n",
    "            )\n",
    "\n",
    "            # Process for each sample time\n",
    "            process_metrics_data_sample(\n",
    "                es_st_r,\n",
    "                es_st_w,\n",
    "                bl_st_ab,\n",
    "                values,\n",
    "                sample_time,\n",
    "                file,\n",
    "                rs,\n",
    "                metrics_reg_list,\n",
    "                metrics_w_list,\n",
    "                metrics_reg_type_list,\n",
    "                metrics_w_type_list,\n",
    "            )\n",
    "\n",
    "    return metrics_reg_list, metrics_w_list, metrics_reg_type_list, metrics_w_type_list\n",
    "\n",
    "\n",
    "# Read data using Polars LazyFrames\n",
    "ground_truth = pl.read_csv(\"prep_out/ground_truth_abundances.csv\")\n",
    "estimate_reg = pl.read_csv(\"prep_out/estimated_abundances_reg.csv\")\n",
    "estimate_w = pl.read_csv(\"prep_out/estimated_abundances_w.csv\")\n",
    "\n",
    "# Convert groupby results into dictionaries for fast lookup\n",
    "ground_truth_dict = {df[\"filename\"][0]: df for df in ground_truth.partition_by(\"filename\")}\n",
    "estimate_reg_dict = {\n",
    "    df[\"filename\"][0]: df for df in estimate_reg.partition_by(\"filename\")\n",
    "}\n",
    "estimate_w_dict = {df[\"filename\"][0]: df for df in estimate_w.partition_by(\"filename\")}\n",
    "\n",
    "metrics_reg_list = []\n",
    "metrics_w_list = []\n",
    "metrics_reg_type_list = []\n",
    "metrics_w_type_list = []\n",
    "\n",
    "metrics_reg, metrics_w, metrics_reg_type, metrics_w_type = process_raw_data(\n",
    "    ground_truth_dict,\n",
    "    estimate_reg_dict,\n",
    "    estimate_w_dict,\n",
    ")\n",
    "metrics_reg_list.extend(metrics_reg)\n",
    "metrics_w_list.extend(metrics_w)\n",
    "metrics_reg_type_list.extend(metrics_reg_type)\n",
    "metrics_w_type_list.extend(metrics_w_type)\n",
    "\n",
    "# Convert the lists to Polars DataFrames\n",
    "metrics_reg_df = pl.DataFrame(metrics_reg_list)\n",
    "metrics_w_df = pl.DataFrame(metrics_w_list)\n",
    "metrics_reg_type_df = pl.DataFrame(metrics_reg_type_list)\n",
    "metrics_w_type_df = pl.DataFrame(metrics_w_type_list)\n",
    "\n",
    "# Save the results as CSV\n",
    "metrics_reg_df.write_csv(\"./analysis_out/abundances_sample_reg.csv\")\n",
    "metrics_w_df.write_csv(\"./analysis_out/abundances_sample_w.csv\")\n",
    "metrics_reg_type_df.write_csv(\"./analysis_out/abundances_sample_reg_type.csv\")\n",
    "metrics_w_type_df.write_csv(\"./analysis_out/abundances_sample_w_type.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N samples sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metrics_data_n_sample(\n",
    "    es_r_r,\n",
    "    es_w_r,\n",
    "    bl_st_ab,\n",
    "    values,\n",
    "    sample_time,\n",
    "    file,\n",
    "    rs,\n",
    "    metrics_reg_list,\n",
    "    metrics_w_list,\n",
    "    metrics_reg_type_list,\n",
    "    metrics_w_type_list,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to process and append metrics for both reg and w data\n",
    "    \"\"\"\n",
    "    for r in rs:\n",
    "        es_r_r_filtered = es_r_r.filter(pl.col(\"r\") == r)\n",
    "        es_w_r_filtered = es_w_r.filter(pl.col(\"r\") == r)\n",
    "\n",
    "        for es, metrics_list, metrics_type_list in [\n",
    "            (es_r_r_filtered, metrics_reg_list, metrics_reg_type_list),\n",
    "            (es_w_r_filtered, metrics_w_list, metrics_w_type_list),\n",
    "        ]:\n",
    "            sample_ids = []\n",
    "            for i in es[\"sample_id\"].unique(maintain_order=True): \n",
    "                sample_ids.append(i)\n",
    "\n",
    "                es_ab = (\n",
    "                    es.filter(pl.col(\"sample_id\").is_in(sample_ids))\n",
    "                    .select(pl.col([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]))\n",
    "                    .sum()\n",
    "                    .to_numpy()\n",
    "                    .flatten()\n",
    "                    / len(sample_ids) # Normalize for # of sample ids\n",
    "                    * 1000\n",
    "                )\n",
    "\n",
    "                # Calculate both MAE (normalized absolute error) and MSE (normalized error)\n",
    "                ae = calculate_normalized_absolute_error([bl_st_ab], es_ab, [values])[0]\n",
    "                mse = calculate_normalized_error([bl_st_ab], es_ab, [values])[0]\n",
    "\n",
    "                metrics_list.append(\n",
    "                    {\n",
    "                        \"filename\": file,\n",
    "                        \"sample_time\": sample_time,\n",
    "                        \"r\": r,\n",
    "                        \"sample_id\": i,\n",
    "                        \"mae\": np.mean(ae),\n",
    "                        \"mdae\": np.median(ae),\n",
    "                        \"mser\": np.mean(mse),\n",
    "                        \"mdser\": np.median(mse),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                for t in range(9):\n",
    "                    es_ab_t = es_ab[t]\n",
    "\n",
    "                    ae = calculate_normalized_absolute_error(\n",
    "                        [bl_st_ab[t]], [es_ab_t], [values[t]]\n",
    "                    )[0]\n",
    "                    ser = calculate_normalized_error(\n",
    "                        [bl_st_ab[t]], [es_ab_t], [values[t]]\n",
    "                    )[0]\n",
    "                    metrics_type_list.append(\n",
    "                        {\n",
    "                            \"filename\": file,\n",
    "                            \"sample_time\": sample_time,\n",
    "                            \"r\": r,\n",
    "                            \"sample_id\": i,\n",
    "                            \"type_id\": t,\n",
    "                            \"ae\": ae,\n",
    "                            \"ser\": ser,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "\n",
    "def process_raw_data(\n",
    "    ground_truth_dict,\n",
    "    estimate_reg_dict,\n",
    "    estimate_w_dict,\n",
    "    rs=[1, 2, 3, 4, 5],\n",
    "    sample_times=[0, 100, 200, 300, 400, 500, 600],\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to process raw data for a given sample time and calculate error metrics\n",
    "    \"\"\"\n",
    "    metrics_reg_list = []\n",
    "    metrics_w_list = []\n",
    "    metrics_reg_type_list = []\n",
    "    metrics_w_type_list = []\n",
    "\n",
    "    # Read raw data, ground_truth, and estimate data for the given file and sample time\n",
    "    for file in ground_truth_dict.keys():\n",
    "        raw_data = pl.read_parquet(f\"./raw_data/{file}.parquet\")\n",
    "        for sample_time in sample_times:\n",
    "            raw_data_st = raw_data.filter(pl.col(\"tick\") == sample_time)\n",
    "            bl = ground_truth_dict[file]\n",
    "            es_r = estimate_reg_dict[file]\n",
    "            es_w = estimate_w_dict[file]\n",
    "\n",
    "            # Precompute unique types and their counts\n",
    "            type_counts = raw_data_st.group_by(\"type\").agg(\n",
    "                pl.col(\"type\").count().alias(\"count\")\n",
    "            )\n",
    "            unique_types = raw_data[\"type\"].unique(maintain_order=True)\n",
    "\n",
    "            # Create a fast lookup array for type counts\n",
    "            values = np.zeros(len(unique_types), dtype=int)\n",
    "            values[np.searchsorted(unique_types, type_counts[\"type\"])] = type_counts[\n",
    "                \"count\"\n",
    "            ].to_numpy()\n",
    "\n",
    "            es_st_r = es_r.filter(pl.col(\"sample_time\") == sample_time)\n",
    "            es_st_w = es_w.filter(pl.col(\"sample_time\") == sample_time)\n",
    "            bl_st = bl.filter(pl.col(\"sample_time\") == sample_time)\n",
    "\n",
    "            bl_st_ab = (\n",
    "                bl_st.select(pl.col([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]))\n",
    "                .to_numpy()\n",
    "                .flatten()\n",
    "                * 1000\n",
    "            )\n",
    "\n",
    "            # Process for each sample time\n",
    "            process_metrics_data_n_sample(\n",
    "                es_st_r,\n",
    "                es_st_w,\n",
    "                bl_st_ab,\n",
    "                values,\n",
    "                sample_time,\n",
    "                file,\n",
    "                rs,\n",
    "                metrics_reg_list,\n",
    "                metrics_w_list,\n",
    "                metrics_reg_type_list,\n",
    "                metrics_w_type_list,\n",
    "            )\n",
    "\n",
    "    return metrics_reg_list, metrics_w_list, metrics_reg_type_list, metrics_w_type_list\n",
    "\n",
    "\n",
    "# Read data using Polars LazyFrames\n",
    "ground_truth = pl.read_csv(\"prep_out/ground_truth_abundances.csv\")\n",
    "estimate_reg = pl.read_csv(\"prep_out/estimated_abundances_reg.csv\")\n",
    "estimate_w = pl.read_csv(\"prep_out/estimated_abundances_w.csv\")\n",
    "\n",
    "# Convert groupby results into dictionaries for fast lookup\n",
    "ground_truth_dict = {df[\"filename\"][0]: df for df in ground_truth.partition_by(\"filename\")}\n",
    "estimate_reg_dict = {\n",
    "    df[\"filename\"][0]: df for df in estimate_reg.partition_by(\"filename\")\n",
    "}\n",
    "estimate_w_dict = {df[\"filename\"][0]: df for df in estimate_w.partition_by(\"filename\")}\n",
    "\n",
    "metrics_reg_list = []\n",
    "metrics_w_list = []\n",
    "metrics_reg_type_list = []\n",
    "metrics_w_type_list = []\n",
    "\n",
    "metrics_reg, metrics_w, metrics_reg_type, metrics_w_type = process_raw_data(\n",
    "    ground_truth_dict,\n",
    "    estimate_reg_dict,\n",
    "    estimate_w_dict,\n",
    ")\n",
    "metrics_reg_list.extend(metrics_reg)\n",
    "metrics_w_list.extend(metrics_w)\n",
    "metrics_reg_type_list.extend(metrics_reg_type)\n",
    "metrics_w_type_list.extend(metrics_w_type)\n",
    "\n",
    "# Convert the lists to Polars DataFrames\n",
    "metrics_reg_df = pl.DataFrame(metrics_reg_list)\n",
    "metrics_w_df = pl.DataFrame(metrics_w_list)\n",
    "metrics_reg_type_df = pl.DataFrame(metrics_reg_type_list)\n",
    "metrics_w_type_df = pl.DataFrame(metrics_w_type_list)\n",
    "\n",
    "# Save the results as CSV\n",
    "metrics_reg_df.write_csv(\"./analysis_out/abundances_n_samples_seq_reg.csv\")\n",
    "metrics_w_df.write_csv(\"./analysis_out/abundances_n_samples_seq_w.csv\")\n",
    "metrics_reg_type_df.write_csv(\"./analysis_out/abundances_n_samples_seq_reg_type.csv\")\n",
    "metrics_w_type_df.write_csv(\"./analysis_out/abundances_n_samples_seq_w_type.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N samples random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metrics_data_n_sample_rand(\n",
    "    es_r_r,\n",
    "    es_w_r,\n",
    "    bl_st_ab,\n",
    "    values,\n",
    "    sample_time,\n",
    "    file,\n",
    "    rs,\n",
    "    metrics_reg_list,\n",
    "    metrics_w_list,\n",
    "    metrics_reg_type_list,\n",
    "    metrics_w_type_list,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to process and append metrics for both reg and w data\n",
    "    \"\"\"\n",
    "    for r in rs:\n",
    "        es_r_r_filtered = es_r_r.filter(pl.col(\"r\") == r)\n",
    "        es_w_r_filtered = es_w_r.filter(pl.col(\"r\") == r)\n",
    "\n",
    "        for es, metrics_list, metrics_type_list in [\n",
    "            (es_r_r_filtered, metrics_reg_list, metrics_reg_type_list),\n",
    "            (es_w_r_filtered, metrics_w_list, metrics_w_type_list),\n",
    "        ]:\n",
    "            for i in range(1, len(es[\"sample_id\"].unique()) + 1): \n",
    "                sample_ids = random.sample(list(es[\"sample_id\"].unique(maintain_order=True)), i)\n",
    "\n",
    "                es_ab = (\n",
    "                    es.filter(pl.col(\"sample_id\").is_in(sample_ids))\n",
    "                    .select(pl.col([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]))\n",
    "                    .sum()\n",
    "                    .to_numpy()\n",
    "                    .flatten()\n",
    "                    / len(sample_ids) # Normalize for # of sample ids\n",
    "                    * 1000\n",
    "                )\n",
    "\n",
    "                # Calculate both MAE (normalized absolute error) and MSE (normalized error)\n",
    "                ae = calculate_normalized_absolute_error([bl_st_ab], es_ab, [values])[0]\n",
    "                mse = calculate_normalized_error([bl_st_ab], es_ab, [values])[0]\n",
    "\n",
    "                metrics_list.append(\n",
    "                    {\n",
    "                        \"filename\": file,\n",
    "                        \"sample_time\": sample_time,\n",
    "                        \"r\": r,\n",
    "                        \"sample_id\": i - 1,\n",
    "                        \"mae\": np.mean(ae),\n",
    "                        \"mdae\": np.median(ae),\n",
    "                        \"mser\": np.mean(mse),\n",
    "                        \"mdser\": np.median(mse),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                for t in range(9):\n",
    "                    es_ab_t = es_ab[t]\n",
    "\n",
    "                    ae = calculate_normalized_absolute_error(\n",
    "                        [bl_st_ab[t]], [es_ab_t], [values[t]]\n",
    "                    )[0]\n",
    "                    ser = calculate_normalized_error(\n",
    "                        [bl_st_ab[t]], [es_ab_t], [values[t]]\n",
    "                    )[0]\n",
    "                    metrics_type_list.append(\n",
    "                        {\n",
    "                            \"filename\": file,\n",
    "                            \"sample_time\": sample_time,\n",
    "                            \"r\": r,\n",
    "                            \"sample_id\": i - 1,\n",
    "                            \"type_id\": t,\n",
    "                            \"ae\": ae,\n",
    "                            \"ser\": ser,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "\n",
    "def process_raw_data(\n",
    "    ground_truth_dict,\n",
    "    estimate_reg_dict,\n",
    "    estimate_w_dict,\n",
    "    rs=[1, 2, 3, 4, 5],\n",
    "    sample_times=[0, 100, 200, 300, 400, 500, 600],\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to process raw data for a given sample time and calculate error metrics\n",
    "    \"\"\"\n",
    "    metrics_reg_list = []\n",
    "    metrics_w_list = []\n",
    "    metrics_reg_type_list = []\n",
    "    metrics_w_type_list = []\n",
    "\n",
    "    # Read raw data, ground_truth, and estimate data for the given file and sample time\n",
    "    for file in ground_truth_dict.keys():\n",
    "        raw_data = pl.read_parquet(f\"./raw_data/{file}.parquet\")\n",
    "        for sample_time in sample_times:\n",
    "            raw_data_st = raw_data.filter(pl.col(\"tick\") == sample_time)\n",
    "            bl = ground_truth_dict[file]\n",
    "            es_r = estimate_reg_dict[file]\n",
    "            es_w = estimate_w_dict[file]\n",
    "\n",
    "            # Precompute unique types and their counts\n",
    "            type_counts = raw_data_st.group_by(\"type\").agg(\n",
    "                pl.col(\"type\").count().alias(\"count\")\n",
    "            )\n",
    "            unique_types = raw_data[\"type\"].unique(maintain_order=True)\n",
    "\n",
    "            # Create a fast lookup array for type counts\n",
    "            values = np.zeros(len(unique_types), dtype=int)\n",
    "            values[np.searchsorted(unique_types, type_counts[\"type\"])] = type_counts[\n",
    "                \"count\"\n",
    "            ].to_numpy()\n",
    "\n",
    "            es_st_r = es_r.filter(pl.col(\"sample_time\") == sample_time)\n",
    "            es_st_w = es_w.filter(pl.col(\"sample_time\") == sample_time)\n",
    "            bl_st = bl.filter(pl.col(\"sample_time\") == sample_time)\n",
    "\n",
    "            bl_st_ab = (\n",
    "                bl_st.select(pl.col([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]))\n",
    "                .to_numpy()\n",
    "                .flatten()\n",
    "                * 1000\n",
    "            )\n",
    "\n",
    "            # Process for each sample time\n",
    "            process_metrics_data_n_sample_rand(\n",
    "                es_st_r,\n",
    "                es_st_w,\n",
    "                bl_st_ab,\n",
    "                values,\n",
    "                sample_time,\n",
    "                file,\n",
    "                rs,\n",
    "                metrics_reg_list,\n",
    "                metrics_w_list,\n",
    "                metrics_reg_type_list,\n",
    "                metrics_w_type_list,\n",
    "            )\n",
    "\n",
    "    return metrics_reg_list, metrics_w_list, metrics_reg_type_list, metrics_w_type_list\n",
    "\n",
    "\n",
    "# Read data using Polars LazyFrames\n",
    "ground_truth = pl.read_csv(\"prep_out/ground_truth_abundances.csv\")\n",
    "estimate_reg = pl.read_csv(\"prep_out/estimated_abundances_reg.csv\")\n",
    "estimate_w = pl.read_csv(\"prep_out/estimated_abundances_w.csv\")\n",
    "\n",
    "# Convert groupby results into dictionaries for fast lookup\n",
    "ground_truth_dict = {df[\"filename\"][0]: df for df in ground_truth.partition_by(\"filename\")}\n",
    "estimate_reg_dict = {\n",
    "    df[\"filename\"][0]: df for df in estimate_reg.partition_by(\"filename\")\n",
    "}\n",
    "estimate_w_dict = {df[\"filename\"][0]: df for df in estimate_w.partition_by(\"filename\")}\n",
    "\n",
    "metrics_reg_list = []\n",
    "metrics_w_list = []\n",
    "metrics_reg_type_list = []\n",
    "metrics_w_type_list = []\n",
    "\n",
    "metrics_reg, metrics_w, metrics_reg_type, metrics_w_type = process_raw_data(\n",
    "    ground_truth_dict,\n",
    "    estimate_reg_dict,\n",
    "    estimate_w_dict,\n",
    ")\n",
    "metrics_reg_list.extend(metrics_reg)\n",
    "metrics_w_list.extend(metrics_w)\n",
    "metrics_reg_type_list.extend(metrics_reg_type)\n",
    "metrics_w_type_list.extend(metrics_w_type)\n",
    "\n",
    "# Convert the lists to Polars DataFrames\n",
    "metrics_reg_df = pl.DataFrame(metrics_reg_list)\n",
    "metrics_w_df = pl.DataFrame(metrics_w_list)\n",
    "metrics_reg_type_df = pl.DataFrame(metrics_reg_type_list)\n",
    "metrics_w_type_df = pl.DataFrame(metrics_w_type_list)\n",
    "\n",
    "# Save the results as CSV\n",
    "metrics_reg_df.write_csv(\"./analysis_out/abundances_n_samples_rand_reg.csv\")\n",
    "metrics_w_df.write_csv(\"./analysis_out/abundances_n_samples_rand_w.csv\")\n",
    "metrics_reg_type_df.write_csv(\"./analysis_out/abundances_n_samples_rand_reg_type.csv\")\n",
    "metrics_w_type_df.write_csv(\"./analysis_out/abundances_n_samples_rand_w_type.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metrics_data_plot(\n",
    "    es_r_r,\n",
    "    es_w_r,\n",
    "    bl_st_ab,\n",
    "    values,\n",
    "    sample_time,\n",
    "    file,\n",
    "    rs,\n",
    "    metrics_reg_list,\n",
    "    metrics_w_list,\n",
    "    metrics_reg_type_list,\n",
    "    metrics_w_type_list,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to process and append metrics for both reg and w data\n",
    "    \"\"\"\n",
    "    for r in rs:\n",
    "        es_r_r_filtered = es_r_r.filter(pl.col(\"r\") == r)\n",
    "        es_w_r_filtered = es_w_r.filter(pl.col(\"r\") == r)\n",
    "\n",
    "        for es, metrics_list, metrics_type_list in [\n",
    "            (es_r_r_filtered, metrics_reg_list, metrics_reg_type_list),\n",
    "            (es_w_r_filtered, metrics_w_list, metrics_w_type_list),\n",
    "        ]:\n",
    "            es_ab = (\n",
    "                es.select(pl.sum([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]))\n",
    "                .to_numpy()\n",
    "                .flatten()\n",
    "                / len(es[\"sample_id\"].unique())\n",
    "                * 1000\n",
    "            )\n",
    "            # Calculate both MAE (normalized absolute error) and MSE (normalized error)\n",
    "            ae = calculate_normalized_absolute_error([bl_st_ab], es_ab, [values])[0]\n",
    "            mse = calculate_normalized_error([bl_st_ab], es_ab, [values])[0]\n",
    "            metrics_list.append(\n",
    "                {\n",
    "                    \"filename\": file,\n",
    "                    \"sample_time\": sample_time,\n",
    "                    \"r\": r,\n",
    "                    \"mae\": np.mean(ae),\n",
    "                    \"mdae\": np.median(ae),\n",
    "                    \"mser\": np.mean(mse),\n",
    "                    \"mdser\": np.median(mse),\n",
    "                }\n",
    "            )\n",
    "            for t in range(9):\n",
    "                es_ab_t = es_ab[t]\n",
    "                ae = calculate_normalized_absolute_error(\n",
    "                    [bl_st_ab[t]], [es_ab_t], [values[t]]\n",
    "                )[0]\n",
    "                ser = calculate_normalized_error([bl_st_ab[t]], [es_ab_t], [values[t]])[\n",
    "                    0\n",
    "                ]\n",
    "                metrics_type_list.append(\n",
    "                    {\n",
    "                        \"filename\": file,\n",
    "                        \"sample_time\": sample_time,\n",
    "                        \"r\": r,\n",
    "                        \"type_id\": t,\n",
    "                        \"ae\": ae,\n",
    "                        \"ser\": ser,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "\n",
    "def process_raw_data(\n",
    "    ground_truth_dict,\n",
    "    estimate_reg_dict,\n",
    "    estimate_w_dict,\n",
    "    rs=[1, 2, 3, 4, 5],\n",
    "    sample_times=[0, 100, 200, 300, 400, 500, 600],\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to process raw data for a given sample time and calculate error metrics\n",
    "    \"\"\"\n",
    "    metrics_reg_list = []\n",
    "    metrics_w_list = []\n",
    "    metrics_reg_type_list = []\n",
    "    metrics_w_type_list = []\n",
    "\n",
    "    # Read raw data, ground_truth, and estimate data for the given file and sample time\n",
    "    for file in ground_truth_dict.keys():\n",
    "        raw_data = pl.read_parquet(f\"./raw_data/{file}.parquet\")\n",
    "        for sample_time in sample_times:\n",
    "            raw_data_st = raw_data.filter(pl.col(\"tick\") == sample_time)\n",
    "            bl = ground_truth_dict[file]\n",
    "            es_r = estimate_reg_dict[file]\n",
    "            es_w = estimate_w_dict[file]\n",
    "\n",
    "            # Precompute unique types and their counts\n",
    "            type_counts = raw_data_st.group_by(\"type\").agg(\n",
    "                pl.col(\"type\").count().alias(\"count\")\n",
    "            )\n",
    "            unique_types = raw_data[\"type\"].unique(maintain_order=True)\n",
    "\n",
    "            # Create a fast lookup array for type counts\n",
    "            values = np.zeros(len(unique_types), dtype=int)\n",
    "            values[np.searchsorted(unique_types, type_counts[\"type\"])] = type_counts[\n",
    "                \"count\"\n",
    "            ].to_numpy()\n",
    "\n",
    "            es_st_r = es_r.filter(pl.col(\"sample_time\") == sample_time)\n",
    "            es_st_w = es_w.filter(pl.col(\"sample_time\") == sample_time)\n",
    "            bl_st = bl.filter(pl.col(\"sample_time\") == sample_time)\n",
    "\n",
    "            bl_st_ab = (\n",
    "                bl_st.select(pl.col([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]))\n",
    "                .to_numpy()\n",
    "                .flatten()\n",
    "                * 1000\n",
    "            )\n",
    "\n",
    "            # Process for each sample time\n",
    "            process_metrics_data_plot(\n",
    "                es_st_r,\n",
    "                es_st_w,\n",
    "                bl_st_ab,\n",
    "                values,\n",
    "                sample_time,\n",
    "                file,\n",
    "                rs,\n",
    "                metrics_reg_list,\n",
    "                metrics_w_list,\n",
    "                metrics_reg_type_list,\n",
    "                metrics_w_type_list,\n",
    "            )\n",
    "\n",
    "    return metrics_reg_list, metrics_w_list, metrics_reg_type_list, metrics_w_type_list\n",
    "\n",
    "\n",
    "# Read data using Polars LazyFrames\n",
    "ground_truth = pl.read_csv(\"prep_out/ground_truth_abundances.csv\")\n",
    "estimate_reg = pl.read_csv(\"prep_out/estimated_abundances_reg.csv\")\n",
    "estimate_w = pl.read_csv(\"prep_out/estimated_abundances_w.csv\")\n",
    "\n",
    "# Convert groupby results into dictionaries for fast lookup\n",
    "ground_truth_dict = {df[\"filename\"][0]: df for df in ground_truth.partition_by(\"filename\")}\n",
    "estimate_reg_dict = {\n",
    "    df[\"filename\"][0]: df for df in estimate_reg.partition_by(\"filename\")\n",
    "}\n",
    "estimate_w_dict = {df[\"filename\"][0]: df for df in estimate_w.partition_by(\"filename\")}\n",
    "\n",
    "metrics_reg_list = []\n",
    "metrics_w_list = []\n",
    "metrics_reg_type_list = []\n",
    "metrics_w_type_list = []\n",
    "\n",
    "# Iterate through sample times and process metrics data\n",
    "\n",
    "metrics_reg, metrics_w, metrics_reg_type, metrics_w_type = process_raw_data(\n",
    "    ground_truth_dict,\n",
    "    estimate_reg_dict,\n",
    "    estimate_w_dict,\n",
    ")\n",
    "metrics_reg_list.extend(metrics_reg)\n",
    "metrics_w_list.extend(metrics_w)\n",
    "metrics_reg_type_list.extend(metrics_reg_type)\n",
    "metrics_w_type_list.extend(metrics_w_type)\n",
    "\n",
    "# Convert the lists to Polars DataFrames\n",
    "metrics_reg_df = pl.DataFrame(metrics_reg_list)\n",
    "metrics_w_df = pl.DataFrame(metrics_w_list)\n",
    "metrics_reg_type_df = pl.DataFrame(metrics_reg_type_list)\n",
    "metrics_w_type_df = pl.DataFrame(metrics_w_type_list)\n",
    "\n",
    "# Save the results as CSV\n",
    "metrics_reg_df.write_csv(\"./analysis_out/abundances_plot_reg.csv\")\n",
    "metrics_w_df.write_csv(\"./analysis_out/abundances_plot_w.csv\")\n",
    "metrics_reg_type_df.write_csv(\"./analysis_out/abundances_plot_reg_type.csv\")\n",
    "metrics_w_type_df.write_csv(\"./analysis_out/abundances_plot_w_type.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metrics_data_temporal(\n",
    "    es_r,\n",
    "    es_w,\n",
    "    bl_ab,\n",
    "    values,\n",
    "    sample_times,\n",
    "    file,\n",
    "    rs,\n",
    "    metrics_reg_list,\n",
    "    metrics_w_list,\n",
    "    metrics_reg_type_list,\n",
    "    metrics_w_type_list,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to process and append metrics for both reg and w data\n",
    "    \"\"\"\n",
    "    for r in rs:\n",
    "        es_r_r = es_r.filter(pl.col(\"r\") == r)\n",
    "        es_w_r = es_w.filter(pl.col(\"r\") == r)\n",
    "\n",
    "        for es, metrics_list, metrics_type_list in [\n",
    "            (es_r_r, metrics_reg_list, metrics_reg_type_list),\n",
    "            (es_w_r, metrics_w_list, metrics_w_type_list),\n",
    "        ]:\n",
    "            es_ab = (\n",
    "                es.select(pl.sum([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]))\n",
    "                .to_numpy()\n",
    "                .flatten()\n",
    "                / len(es[\"sample_id\"].unique())\n",
    "                / len(sample_times)\n",
    "                * 1000\n",
    "            )\n",
    "            # Calculate both MAE (normalized absolute error) and MSE (normalized error)\n",
    "            ae = calculate_normalized_absolute_error([bl_ab], es_ab, [values])[0]\n",
    "            mse = calculate_normalized_error([bl_ab], es_ab, [values])[0]\n",
    "            metrics_list.append(\n",
    "                {\n",
    "                    \"filename\": file,\n",
    "                    \"r\": r,\n",
    "                    \"mae\": np.mean(ae),\n",
    "                    \"mdae\": np.median(ae),\n",
    "                    \"mser\": np.mean(mse),\n",
    "                    \"mdser\": np.median(mse),\n",
    "                }\n",
    "            )\n",
    "            for t in range(9):\n",
    "                es_ab_t = es_ab[t]\n",
    "                ae = calculate_normalized_absolute_error(\n",
    "                    [bl_ab[t]], [es_ab_t], [values[t]]\n",
    "                )[0]\n",
    "                ser = calculate_normalized_error([bl_ab[t]], [es_ab_t], [values[t]])[0]\n",
    "                metrics_type_list.append(\n",
    "                    {\n",
    "                        \"filename\": file,\n",
    "                        \"r\": r,\n",
    "                        \"type_id\": t,\n",
    "                        \"ae\": ae,\n",
    "                        \"ser\": ser,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "\n",
    "def process_raw_data(\n",
    "    ground_truth_dict,\n",
    "    estimate_reg_dict,\n",
    "    estimate_w_dict,\n",
    "    rs=[1, 2, 3, 4, 5],\n",
    "    sample_times=[0, 100, 200, 300, 400, 500, 600],\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to process raw data for a given sample time and calculate error metrics\n",
    "    \"\"\"\n",
    "    metrics_reg_list = []\n",
    "    metrics_w_list = []\n",
    "    metrics_reg_type_list = []\n",
    "    metrics_w_type_list = []\n",
    "\n",
    "    # Read raw data, ground_truth, and estimate data for the given file and sample time\n",
    "    for file in ground_truth_dict.keys():\n",
    "        raw_data = pl.read_parquet(f\"./raw_data/{file}.parquet\")\n",
    "        raw_data_st = raw_data.filter(pl.col(\"tick\").is_in(sample_times))\n",
    "        bl = ground_truth_dict[file]\n",
    "        es_r = estimate_reg_dict[file]\n",
    "        es_w = estimate_w_dict[file]\n",
    "        # Precompute unique types and their counts\n",
    "        type_counts = raw_data_st.group_by(\"type\").agg(\n",
    "            pl.col(\"type\").count().alias(\"count\")\n",
    "        )\n",
    "        unique_types = raw_data[\"type\"].unique(maintain_order=True)\n",
    "        # Create a fast lookup array for type counts\n",
    "        values = np.zeros(len(unique_types), dtype=int)\n",
    "        values[np.searchsorted(unique_types, type_counts[\"type\"])] = type_counts[\n",
    "            \"count\"\n",
    "        ].to_numpy()\n",
    "\n",
    "        values = values.astype(float)\n",
    "        values /= len(sample_times)\n",
    "\n",
    "        bl_ab = (\n",
    "            bl.select(pl.sum([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]))\n",
    "            .to_numpy()\n",
    "            .flatten()\n",
    "            / len(sample_times)\n",
    "            * 1000\n",
    "        )\n",
    "        # Process for each sample time\n",
    "        process_metrics_data_temporal(\n",
    "            es_r,\n",
    "            es_w,\n",
    "            bl_ab,\n",
    "            values,\n",
    "            sample_times,\n",
    "            file,\n",
    "            rs,\n",
    "            metrics_reg_list,\n",
    "            metrics_w_list,\n",
    "            metrics_reg_type_list,\n",
    "            metrics_w_type_list,\n",
    "        )\n",
    "\n",
    "    return metrics_reg_list, metrics_w_list, metrics_reg_type_list, metrics_w_type_list\n",
    "\n",
    "\n",
    "# Read data using Polars LazyFrames\n",
    "ground_truth = pl.read_csv(\"prep_out/ground_truth_abundances.csv\")\n",
    "estimate_reg = pl.read_csv(\"prep_out/estimated_abundances_reg.csv\")\n",
    "estimate_w = pl.read_csv(\"prep_out/estimated_abundances_w.csv\")\n",
    "\n",
    "# Convert groupby results into dictionaries for fast lookup\n",
    "ground_truth_dict = {df[\"filename\"][0]: df for df in ground_truth.partition_by(\"filename\")}\n",
    "estimate_reg_dict = {\n",
    "    df[\"filename\"][0]: df for df in estimate_reg.partition_by(\"filename\")\n",
    "}\n",
    "estimate_w_dict = {df[\"filename\"][0]: df for df in estimate_w.partition_by(\"filename\")}\n",
    "\n",
    "metrics_reg_list = []\n",
    "metrics_w_list = []\n",
    "metrics_reg_type_list = []\n",
    "metrics_w_type_list = []\n",
    "\n",
    "# Iterate through sample times and process metrics data\n",
    "\n",
    "metrics_reg, metrics_w, metrics_reg_type, metrics_w_type = process_raw_data(\n",
    "    ground_truth_dict,\n",
    "    estimate_reg_dict,\n",
    "    estimate_w_dict,\n",
    ")\n",
    "metrics_reg_list.extend(metrics_reg)\n",
    "metrics_w_list.extend(metrics_w)\n",
    "metrics_reg_type_list.extend(metrics_reg_type)\n",
    "metrics_w_type_list.extend(metrics_w_type)\n",
    "\n",
    "# Convert the lists to Polars DataFrames\n",
    "metrics_reg_df = pl.DataFrame(metrics_reg_list)\n",
    "metrics_w_df = pl.DataFrame(metrics_w_list)\n",
    "metrics_reg_type_df = pl.DataFrame(metrics_reg_type_list)\n",
    "metrics_w_type_df = pl.DataFrame(metrics_w_type_list)\n",
    "\n",
    "# Save the results as CSV\n",
    "metrics_reg_df.write_csv(\"./analysis_out/abundances_temporal_reg.csv\")\n",
    "metrics_w_df.write_csv(\"./analysis_out/abundances_temporal_w.csv\")\n",
    "metrics_reg_type_df.write_csv(\"./analysis_out/abundances_temporal_reg_type.csv\")\n",
    "metrics_w_type_df.write_csv(\"./analysis_out/abundances_temporal_w_type.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diversity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSVs using Polars\n",
    "ground_truth = pl.read_csv(\"prep_out/ground_truth_diversity_indices.csv\")\n",
    "estimate_reg = pl.read_csv(\"prep_out/estimated_diversity_indices_sample_reg.csv\")\n",
    "estimate_w = pl.read_csv(\"prep_out/estimated_diversity_indices_sample_w.csv\")\n",
    "\n",
    "rs = [1, 2, 3, 4, 5]\n",
    "sample_times = [0, 100, 200, 300, 400, 500, 600]\n",
    "\n",
    "metrics_list_reg = []\n",
    "metrics_list_w = []\n",
    "\n",
    "for file in ground_truth[\"filename\"].unique(maintain_order=True):\n",
    "    bl_f = ground_truth.filter(pl.col(\"filename\") == file)\n",
    "    es_r = estimate_reg.filter(pl.col(\"filename\") == file)\n",
    "    es_w = estimate_w.filter(pl.col(\"filename\") == file)\n",
    "\n",
    "    for st in sample_times:\n",
    "        bl_st = bl_f.filter(pl.col(\"sample_time\") == st)\n",
    "        bl_sh, bl_si = bl_st.select([\"shannon\", \"simpson\"]).row(0)\n",
    "\n",
    "        es_st_r = es_r.filter(pl.col(\"sample_time\") == st)\n",
    "        es_st_w = es_w.filter(pl.col(\"sample_time\") == st)\n",
    "\n",
    "        for r in rs:\n",
    "            es_r_r = es_st_r.filter(pl.col(\"r\") == r)\n",
    "            es_w_r = es_st_w.filter(pl.col(\"r\") == r)\n",
    "\n",
    "            for sample_id in es_r_r[\"sample_id\"].unique(maintain_order=True):\n",
    "                es_r_s = es_r_r.filter(pl.col(\"sample_id\") == sample_id)\n",
    "                sh, si = es_r_s.select([\"shannon\", \"simpson\"]).row(0)\n",
    "\n",
    "                metrics_list_reg.append(\n",
    "                    {\n",
    "                        \"filename\": file,\n",
    "                        \"r\": r,\n",
    "                        \"sample_time\": st,\n",
    "                        \"sample_id\": sample_id,\n",
    "                        \"mae_sh\": mean_absolute_error([bl_sh], [sh]),\n",
    "                        \"mae_si\": mean_absolute_error([bl_si], [si]),\n",
    "                        \"mdae_sh\": median_absolute_error([bl_sh], [sh]),\n",
    "                        \"mdae_si\": median_absolute_error([bl_si], [si]),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            for sample_id in es_w_r[\"sample_id\"].unique(maintain_order=True):\n",
    "                es_w_s = es_w_r.filter(pl.col(\"sample_id\") == sample_id)\n",
    "                sh, si = es_w_s.select([\"shannon\", \"simpson\"]).row(0)\n",
    "\n",
    "                metrics_list_w.append(\n",
    "                    {\n",
    "                        \"filename\": file,\n",
    "                        \"r\": r,\n",
    "                        \"sample_time\": st,\n",
    "                        \"sample_id\": sample_id,\n",
    "                        \"mae_sh\": mean_absolute_error([bl_sh], [sh]),\n",
    "                        \"mae_si\": mean_absolute_error([bl_si], [si]),\n",
    "                        \"mdae_sh\": median_absolute_error([bl_sh], [sh]),\n",
    "                        \"mdae_si\": median_absolute_error([bl_si], [si]),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "# Convert lists to Polars DataFrame and write to CSV\n",
    "pl.DataFrame(metrics_list_reg).write_csv(\n",
    "    \"./analysis_out/diversity_indices_sample_reg.csv\"\n",
    ")\n",
    "pl.DataFrame(metrics_list_w).write_csv(\n",
    "    \"./analysis_out/diversity_indices_sample_w.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N Sample seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSVs using Polars\n",
    "ground_truth = pl.read_csv(\"prep_out/ground_truth_diversity_indices.csv\")\n",
    "estimate_reg = pl.read_csv(\"prep_out/estimated_diversity_indices_n_samples_seq_reg.csv\")\n",
    "estimate_w = pl.read_csv(\"prep_out/estimated_diversity_indices_n_samples_seq_w.csv\")\n",
    "\n",
    "rs = [1, 2, 3, 4, 5]\n",
    "sample_times = [0, 100, 200, 300, 400, 500, 600]\n",
    "\n",
    "metrics_list_reg = []\n",
    "metrics_list_w = []\n",
    "\n",
    "for file in ground_truth[\"filename\"].unique(maintain_order=True):\n",
    "    bl_f = ground_truth.filter(pl.col(\"filename\") == file)\n",
    "    es_r = estimate_reg.filter(pl.col(\"filename\") == file)\n",
    "    es_w = estimate_w.filter(pl.col(\"filename\") == file)\n",
    "\n",
    "    for st in sample_times:\n",
    "        bl_st = bl_f.filter(pl.col(\"sample_time\") == st)\n",
    "        bl_sh, bl_si = bl_st.select([\"shannon\", \"simpson\"]).row(0)\n",
    "\n",
    "        es_st_r = es_r.filter(pl.col(\"sample_time\") == st)\n",
    "        es_st_w = es_w.filter(pl.col(\"sample_time\") == st)\n",
    "\n",
    "        for r in rs:\n",
    "            es_r_r = es_st_r.filter(pl.col(\"r\") == r)\n",
    "            es_w_r = es_st_w.filter(pl.col(\"r\") == r)\n",
    "\n",
    "            for sample_id in es_r_r[\"sample_id\"].unique(maintain_order=True):\n",
    "                es_r_s = es_r_r.filter(pl.col(\"sample_id\") == sample_id)\n",
    "                sh, si = es_r_s.select([\"shannon\", \"simpson\"]).row(0)\n",
    "\n",
    "                metrics_list_reg.append(\n",
    "                    {\n",
    "                        \"filename\": file,\n",
    "                        \"r\": r,\n",
    "                        \"sample_time\": st,\n",
    "                        \"sample_id\": sample_id,\n",
    "                        \"mae_sh\": mean_absolute_error([bl_sh], [sh]),\n",
    "                        \"mae_si\": mean_absolute_error([bl_si], [si]),\n",
    "                        \"mdae_sh\": median_absolute_error([bl_sh], [sh]),\n",
    "                        \"mdae_si\": median_absolute_error([bl_si], [si]),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            for sample_id in es_w_r[\"sample_id\"].unique(maintain_order=True):\n",
    "                es_w_s = es_w_r.filter(pl.col(\"sample_id\") == sample_id)\n",
    "                sh, si = es_w_s.select([\"shannon\", \"simpson\"]).row(0)\n",
    "\n",
    "                metrics_list_w.append(\n",
    "                    {\n",
    "                        \"filename\": file,\n",
    "                        \"r\": r,\n",
    "                        \"sample_time\": st,\n",
    "                        \"sample_id\": sample_id,\n",
    "                        \"mae_sh\": mean_absolute_error([bl_sh], [sh]),\n",
    "                        \"mae_si\": mean_absolute_error([bl_si], [si]),\n",
    "                        \"mdae_sh\": median_absolute_error([bl_sh], [sh]),\n",
    "                        \"mdae_si\": median_absolute_error([bl_si], [si]),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "# Convert lists to Polars DataFrame and write to CSV\n",
    "pl.DataFrame(metrics_list_reg).write_csv(\n",
    "    \"./analysis_out/diversity_indices_n_samples_seq_reg.csv\"\n",
    ")\n",
    "pl.DataFrame(metrics_list_w).write_csv(\n",
    "    \"./analysis_out/diversity_indices_n_samples_seq_w.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N Sample rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSVs using Polars\n",
    "ground_truth = pl.read_csv(\"prep_out/ground_truth_diversity_indices.csv\")\n",
    "estimate_reg = pl.read_csv(\"prep_out/estimated_diversity_indices_n_samples_rand_reg.csv\")\n",
    "estimate_w = pl.read_csv(\"prep_out/estimated_diversity_indices_n_samples_rand_w.csv\")\n",
    "\n",
    "rs = [1, 2, 3, 4, 5]\n",
    "sample_times = [0, 100, 200, 300, 400, 500, 600]\n",
    "\n",
    "metrics_list_reg = []\n",
    "metrics_list_w = []\n",
    "\n",
    "for file in ground_truth[\"filename\"].unique(maintain_order=True):\n",
    "    bl_f = ground_truth.filter(pl.col(\"filename\") == file)\n",
    "    es_r = estimate_reg.filter(pl.col(\"filename\") == file)\n",
    "    es_w = estimate_w.filter(pl.col(\"filename\") == file)\n",
    "\n",
    "    for st in sample_times:\n",
    "        bl_st = bl_f.filter(pl.col(\"sample_time\") == st)\n",
    "        bl_sh, bl_si = bl_st.select([\"shannon\", \"simpson\"]).row(0)\n",
    "\n",
    "        es_st_r = es_r.filter(pl.col(\"sample_time\") == st)\n",
    "        es_st_w = es_w.filter(pl.col(\"sample_time\") == st)\n",
    "\n",
    "        for r in rs:\n",
    "            es_r_r = es_st_r.filter(pl.col(\"r\") == r)\n",
    "            es_w_r = es_st_w.filter(pl.col(\"r\") == r)\n",
    "\n",
    "            for sample_id in es_r_r[\"sample_id\"].unique(maintain_order=True):\n",
    "                es_r_s = es_r_r.filter(pl.col(\"sample_id\") == sample_id)\n",
    "                sh, si = es_r_s.select([\"shannon\", \"simpson\"]).row(0)\n",
    "\n",
    "                metrics_list_reg.append(\n",
    "                    {\n",
    "                        \"filename\": file,\n",
    "                        \"r\": r,\n",
    "                        \"sample_time\": st,\n",
    "                        \"sample_id\": sample_id,\n",
    "                        \"mae_sh\": mean_absolute_error([bl_sh], [sh]),\n",
    "                        \"mae_si\": mean_absolute_error([bl_si], [si]),\n",
    "                        \"mdae_sh\": median_absolute_error([bl_sh], [sh]),\n",
    "                        \"mdae_si\": median_absolute_error([bl_si], [si]),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            for sample_id in es_w_r[\"sample_id\"].unique(maintain_order=True):\n",
    "                es_w_s = es_w_r.filter(pl.col(\"sample_id\") == sample_id)\n",
    "                sh, si = es_w_s.select([\"shannon\", \"simpson\"]).row(0)\n",
    "\n",
    "                metrics_list_w.append(\n",
    "                    {\n",
    "                        \"filename\": file,\n",
    "                        \"r\": r,\n",
    "                        \"sample_time\": st,\n",
    "                        \"sample_id\": sample_id,\n",
    "                        \"mae_sh\": mean_absolute_error([bl_sh], [sh]),\n",
    "                        \"mae_si\": mean_absolute_error([bl_si], [si]),\n",
    "                        \"mdae_sh\": median_absolute_error([bl_sh], [sh]),\n",
    "                        \"mdae_si\": median_absolute_error([bl_si], [si]),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "# Convert lists to Polars DataFrame and write to CSV\n",
    "pl.DataFrame(metrics_list_reg).write_csv(\n",
    "    \"./analysis_out/diversity_indices_n_samples_rand_reg.csv\"\n",
    ")\n",
    "pl.DataFrame(metrics_list_w).write_csv(\n",
    "    \"./analysis_out/diversity_indices_n_samples_rand_w.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pl.read_csv(\"prep_out/ground_truth_diversity_indices.csv\")\n",
    "estimate_reg = pl.read_csv(\"prep_out/estimated_diversity_indices_plot_reg.csv\")\n",
    "estimate_w = pl.read_csv(\"prep_out/estimated_diversity_indices_plot_w.csv\")\n",
    "\n",
    "rs = [1, 2, 3, 4, 5]\n",
    "sample_times = [0, 100, 200, 300, 400, 500, 600]\n",
    "\n",
    "metrics_list_reg = []\n",
    "metrics_list_w = []\n",
    "\n",
    "for file in ground_truth[\"filename\"].unique(maintain_order=True):\n",
    "    bl_f = ground_truth.filter(pl.col(\"filename\") == file)\n",
    "    es_r = estimate_reg.filter(pl.col(\"filename\") == file)\n",
    "    es_w = estimate_w.filter(pl.col(\"filename\") == file)\n",
    "\n",
    "    for st in sample_times:\n",
    "        bl_st = bl_f.filter(pl.col(\"sample_time\") == st)\n",
    "        bl_sh, bl_si = bl_st.select([\"shannon\", \"simpson\"]).row(0)\n",
    "\n",
    "        es_st_r = es_r.filter(pl.col(\"sample_time\") == st)\n",
    "        es_st_w = es_w.filter(pl.col(\"sample_time\") == st)\n",
    "\n",
    "        for r in rs:\n",
    "            es_r_r = es_st_r.filter(pl.col(\"r\") == r)\n",
    "            es_w_r = es_st_w.filter(pl.col(\"r\") == r)\n",
    "\n",
    "            sh, si = es_r_r.select([\"shannon\", \"simpson\"]).row(0)\n",
    "            metrics_list_reg.append(\n",
    "                {\n",
    "                    \"filename\": file,\n",
    "                    \"r\": r,\n",
    "                    \"sample_time\": st,\n",
    "                    \"mae_sh\": mean_absolute_error([bl_sh], [sh]),\n",
    "                    \"mae_si\": mean_absolute_error([bl_si], [si]),\n",
    "                    \"mdae_sh\": median_absolute_error([bl_sh], [sh]),\n",
    "                    \"mdae_si\": median_absolute_error([bl_si], [si]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            sh, si = es_w_r.select([\"shannon\", \"simpson\"]).row(0)\n",
    "            metrics_list_w.append(\n",
    "                {\n",
    "                    \"filename\": file,\n",
    "                    \"r\": r,\n",
    "                    \"sample_time\": st,\n",
    "                    \"mae_sh\": mean_absolute_error([bl_sh], [sh]),\n",
    "                    \"mae_si\": mean_absolute_error([bl_si], [si]),\n",
    "                    \"mdae_sh\": median_absolute_error([bl_sh], [sh]),\n",
    "                    \"mdae_si\": median_absolute_error([bl_si], [si]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "# Convert lists to Polars DataFrame and write to CSV\n",
    "pl.DataFrame(metrics_list_reg).write_csv(\n",
    "    \"./analysis_out/diversity_indices_plot_reg.csv\"\n",
    ")\n",
    "pl.DataFrame(metrics_list_w).write_csv(\n",
    "    \"./analysis_out/diversity_indices_plot_w.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSVs using Polars\n",
    "ground_truth = pl.read_csv(\"prep_out/ground_truth_diversity_indices.csv\")\n",
    "estimate_reg = pl.read_csv(\"prep_out/estimated_diversity_indices_temporal_reg.csv\")\n",
    "estimate_w = pl.read_csv(\"prep_out/estimated_diversity_indices_temporal_w.csv\")\n",
    "\n",
    "rs = [1, 2, 3, 4, 5]\n",
    "\n",
    "metrics_list_reg = []\n",
    "metrics_list_w = []\n",
    "\n",
    "for file in ground_truth[\"filename\"].unique(maintain_order=True):\n",
    "    bl_f = ground_truth.filter(pl.col(\"filename\") == file)\n",
    "    bl_sh = bl_f[\"shannon\"].mean()\n",
    "    bl_si = bl_f[\"simpson\"].mean()\n",
    "\n",
    "    es_r = estimate_reg.filter(pl.col(\"filename\") == file)\n",
    "    es_w = estimate_w.filter(pl.col(\"filename\") == file)\n",
    "\n",
    "    for r in rs:\n",
    "        es_r_r = es_r.filter(pl.col(\"r\") == r)\n",
    "        es_w_r = es_w.filter(pl.col(\"r\") == r)\n",
    "\n",
    "        sh, si = es_r_r.select([\"shannon\", \"simpson\"]).row(0)\n",
    "        metrics_list_reg.append(\n",
    "            {\n",
    "                \"filename\": file,\n",
    "                \"r\": r,\n",
    "                \"mae_sh\": mean_absolute_error([bl_sh], [sh]),\n",
    "                \"mae_si\": mean_absolute_error([bl_si], [si]),\n",
    "                \"mdae_sh\": median_absolute_error([bl_sh], [sh]),\n",
    "                \"mdae_si\": median_absolute_error([bl_si], [si]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        sh, si = es_w_r.select([\"shannon\", \"simpson\"]).row(0)\n",
    "        metrics_list_w.append(\n",
    "            {\n",
    "                \"filename\": file,\n",
    "                \"r\": r,\n",
    "                \"mae_sh\": mean_absolute_error([bl_sh], [sh]),\n",
    "                \"mae_si\": mean_absolute_error([bl_si], [si]),\n",
    "                \"mdae_sh\": median_absolute_error([bl_sh], [sh]),\n",
    "                \"mdae_si\": median_absolute_error([bl_si], [si]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert lists to Polars DataFrame and write to CSV\n",
    "pl.DataFrame(metrics_list_reg).write_csv(\n",
    "    \"./analysis_out/diversity_indices_temporal_reg.csv\"\n",
    ")\n",
    "pl.DataFrame(metrics_list_w).write_csv(\n",
    "    \"./analysis_out/diversity_indices_temporal_w.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D-Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pl.read_csv(\"prep_out/ground_truth_d_index.csv\")\n",
    "estimate_reg = pl.read_csv(\"prep_out/estimated_d_index_sample_reg.csv\")\n",
    "estimate_w = pl.read_csv(\"prep_out/estimated_d_index_sample_w.csv\")\n",
    "\n",
    "rs = [1, 2, 3, 4, 5]\n",
    "sample_times = [0, 100, 200, 300, 400, 500, 600]\n",
    "\n",
    "metrics_reg = []\n",
    "metrics_w = []\n",
    "\n",
    "# Process each unique filename\n",
    "for file in ground_truth[\"filename\"].unique(maintain_order=True):\n",
    "    bl_f = ground_truth.filter(pl.col(\"filename\") == file)\n",
    "    es_r = estimate_reg.filter(pl.col(\"filename\") == file)\n",
    "    es_w = estimate_w.filter(pl.col(\"filename\") == file)\n",
    "\n",
    "    for t in ground_truth[\"type_id\"].unique(maintain_order=True):\n",
    "        bl_t = bl_f.filter(pl.col(\"type_id\") == t)\n",
    "        es_r_t = es_r.filter(pl.col(\"type_id\") == t)\n",
    "        es_w_t = es_w.filter(pl.col(\"type_id\") == t)\n",
    "\n",
    "        for st in sample_times:\n",
    "            es_st_r = es_r_t.filter(pl.col(\"sample_time\") == st)\n",
    "            es_st_w = es_w_t.filter(pl.col(\"sample_time\") == st)\n",
    "\n",
    "            bl_st = (\n",
    "                bl_t.filter(pl.col(\"sample_time\") == st)\n",
    "                .select([str(i) for i in range(9)])\n",
    "                .to_numpy()\n",
    "                .flatten()\n",
    "            )\n",
    "\n",
    "            for sample_id in es_st_r[\"sample_id\"].unique(maintain_order=True):\n",
    "                es_r_s = es_st_r.filter(pl.col(\"sample_id\") == sample_id)\n",
    "\n",
    "                for r in rs:\n",
    "                    es_r_r = (\n",
    "                        es_r_s.filter(pl.col(\"r\") == r)\n",
    "                        .select([str(i) for i in range(9)])\n",
    "                        .to_numpy()\n",
    "                        .flatten()\n",
    "                    )\n",
    "\n",
    "                    metrics_reg.append(\n",
    "                        {\n",
    "                            \"filename\": file,\n",
    "                            \"sample_time\": st,\n",
    "                            \"r\": r,\n",
    "                            \"sample_id\": sample_id,\n",
    "                            \"type_id\": t,\n",
    "                            \"mae\": mean_absolute_error(bl_st, es_r_r),\n",
    "                            \"mdae\": median_absolute_error(bl_st, es_r_r),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            for sample_id in es_st_w[\"sample_id\"].unique(maintain_order=True):\n",
    "                es_w_s = es_st_w.filter(pl.col(\"sample_id\") == sample_id)\n",
    "\n",
    "                for r in rs:\n",
    "                    es_w_r = (\n",
    "                        es_w_s.filter(pl.col(\"r\") == r)\n",
    "                        .select([str(i) for i in range(9)])\n",
    "                        .to_numpy()\n",
    "                        .flatten()\n",
    "                    )\n",
    "\n",
    "                    metrics_w.append(\n",
    "                        {\n",
    "                            \"filename\": file,\n",
    "                            \"sample_time\": st,\n",
    "                            \"r\": r,\n",
    "                            \"sample_id\": sample_id,\n",
    "                            \"type_id\": t,\n",
    "                            \"mae\": mean_absolute_error(bl_st, es_w_r),\n",
    "                            \"mdae\": median_absolute_error(bl_st, es_w_r),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "# Convert lists to Polars DataFrame and save\n",
    "pl.DataFrame(metrics_reg).write_csv(\"./analysis_out/d_index_sample_reg.csv\")\n",
    "pl.DataFrame(metrics_w).write_csv(\"./analysis_out/d_index_sample_w.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pl.read_csv(\"prep_out/ground_truth_d_index.csv\")\n",
    "estimate_reg = pl.read_csv(\"prep_out/estimated_d_index_plot_reg.csv\")\n",
    "estimate_w = pl.read_csv(\"prep_out/estimated_d_index_plot_w.csv\")\n",
    "\n",
    "rs = [1, 2, 3, 4, 5]\n",
    "sample_times = [0, 100, 200, 300, 400, 500, 600]\n",
    "\n",
    "# Initialize lists for storing results\n",
    "metrics_reg_list = []\n",
    "metrics_w_list = []\n",
    "\n",
    "# Iterate over unique filenames\n",
    "for file in ground_truth[\"filename\"].unique(maintain_order=True):\n",
    "    bl_f = ground_truth.filter(pl.col(\"filename\") == file)\n",
    "    es_r = estimate_reg.filter(pl.col(\"filename\") == file)\n",
    "    es_w = estimate_w.filter(pl.col(\"filename\") == file)\n",
    "\n",
    "    for st in sample_times:\n",
    "        es_st_r = es_r.filter(pl.col(\"sample_time\") == st)\n",
    "        es_st_w = es_w.filter(pl.col(\"sample_time\") == st)\n",
    "        bl_st = bl_f.filter(pl.col(\"sample_time\") == st)\n",
    "\n",
    "        for t in ground_truth[\"type_id\"].unique(maintain_order=True):\n",
    "            bl = (\n",
    "                bl_st.filter(pl.col(\"type_id\") == t)\n",
    "                .select([str(i) for i in range(9)])\n",
    "                .to_numpy()\n",
    "                .flatten()\n",
    "            )\n",
    "\n",
    "            es_r_t = es_st_r.filter(pl.col(\"type_id\") == t)\n",
    "            es_w_t = es_st_w.filter(pl.col(\"type_id\") == t)\n",
    "\n",
    "            for r in rs:\n",
    "                es_r_r = (\n",
    "                    es_r_t.filter(pl.col(\"r\") == r)\n",
    "                    .select([str(i) for i in range(9)])\n",
    "                    .to_numpy()\n",
    "                    .flatten()\n",
    "                )\n",
    "                es_w_r = (\n",
    "                    es_w_t.filter(pl.col(\"r\") == r)\n",
    "                    .select([str(i) for i in range(9)])\n",
    "                    .to_numpy()\n",
    "                    .flatten()\n",
    "                )\n",
    "\n",
    "                # Compute metrics and store in list\n",
    "                metrics_reg_list.append(\n",
    "                    {\n",
    "                        \"filename\": file,\n",
    "                        \"sample_time\": st,\n",
    "                        \"r\": r,\n",
    "                        \"type_id\": t,\n",
    "                        \"mae\": mean_absolute_error(bl, es_r_r),\n",
    "                        \"mdae\": median_absolute_error(bl, es_r_r),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                metrics_w_list.append(\n",
    "                    {\n",
    "                        \"filename\": file,\n",
    "                        \"sample_time\": st,\n",
    "                        \"r\": r,\n",
    "                        \"type_id\": t,\n",
    "                        \"mae\": mean_absolute_error(bl, es_w_r),\n",
    "                        \"mdae\": median_absolute_error(bl, es_w_r),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "# Convert lists to Polars DataFrames\n",
    "metrics_reg = pl.DataFrame(metrics_reg_list)\n",
    "metrics_w = pl.DataFrame(metrics_w_list)\n",
    "\n",
    "# Save to CSV\n",
    "metrics_reg.write_csv(\"./analysis_out/d_index_plot_reg.csv\")\n",
    "metrics_w.write_csv(\"./analysis_out/d_index_plot_w.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pl.read_csv(\"prep_out/ground_truth_d_index.csv\")\n",
    "estimate_reg = pl.read_csv(\"prep_out/estimated_d_index_temporal_reg.csv\")\n",
    "estimate_w = pl.read_csv(\"prep_out/estimated_d_index_temporal_w.csv\")\n",
    "\n",
    "rs = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Initialize lists for storing results\n",
    "metrics_reg_list = []\n",
    "metrics_w_list = []\n",
    "\n",
    "# Iterate over unique filenames\n",
    "for file in ground_truth[\"filename\"].unique(maintain_order=True):\n",
    "    bl_f = ground_truth.filter(pl.col(\"filename\") == file)\n",
    "    es_r = estimate_reg.filter(pl.col(\"filename\") == file)\n",
    "    es_w = estimate_w.filter(pl.col(\"filename\") == file)\n",
    "\n",
    "    for t in ground_truth[\"type_id\"].unique(maintain_order=True):\n",
    "        # Compute mean of ground_truth for type_id == 0\n",
    "        bl = (\n",
    "            bl_f.filter(pl.col(\"type_id\") == 0)\n",
    "            .select([str(i) for i in range(9)])\n",
    "            .mean()\n",
    "            .to_numpy()\n",
    "            .flatten()\n",
    "        )\n",
    "\n",
    "        es_r_t = es_r.filter(pl.col(\"type_id\") == t)\n",
    "        es_w_t = es_w.filter(pl.col(\"type_id\") == t)\n",
    "\n",
    "        for r in rs:\n",
    "            # Compute mean for each r\n",
    "            es_r_r = (\n",
    "                es_r_t.filter(pl.col(\"r\") == r)\n",
    "                .select([str(i) for i in range(9)])\n",
    "                .mean()\n",
    "                .to_numpy()\n",
    "                .flatten()\n",
    "            )\n",
    "            es_w_r = (\n",
    "                es_w_t.filter(pl.col(\"r\") == r)\n",
    "                .select([str(i) for i in range(9)])\n",
    "                .mean()\n",
    "                .to_numpy()\n",
    "                .flatten()\n",
    "            )\n",
    "\n",
    "            # Compute metrics and store in list\n",
    "            metrics_reg_list.append(\n",
    "                {\n",
    "                    \"filename\": file,\n",
    "                    \"r\": r,\n",
    "                    \"type_id\": t,\n",
    "                    \"mae\": mean_absolute_error(bl, es_r_r),\n",
    "                    \"mdae\": median_absolute_error(bl, es_r_r),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            metrics_w_list.append(\n",
    "                {\n",
    "                    \"filename\": file,\n",
    "                    \"r\": r,\n",
    "                    \"type_id\": t,\n",
    "                    \"mae\": mean_absolute_error(bl, es_w_r),\n",
    "                    \"mdae\": median_absolute_error(bl, es_w_r),\n",
    "                }\n",
    "            )\n",
    "\n",
    "# Convert lists to Polars DataFrames\n",
    "metrics_reg = pl.DataFrame(metrics_reg_list)\n",
    "metrics_w = pl.DataFrame(metrics_w_list)\n",
    "\n",
    "# Save to CSV\n",
    "metrics_reg.write_csv(\"./analysis_out/d_index_temporal_reg.csv\")\n",
    "metrics_w.write_csv(\"./analysis_out/d_index_temporal_w.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blossom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
